# Tinygrad Experiments

[Tinygrad](https://github.com/tinygrad/tinygrad) is cool... or so I've heard. Going to mess around with it here. Will update.

Goal is to get a basic understanding of how it works, and then try to contribute somehow to the project.

Update submodules:
    `git submodule update --remote`

## TODO
- [ ] follow the basic MLP example from the tinygrad README, get it training
- [ ] train a model on MNIST or something
- [ ] work through all the docs

## Hopes and Dreams
- [ ] reverse engineer and annotate the transformer example
- [ ] try to implement a transformer from scratch
  - [ ] add something cool like sliding window attention, flash attention, rotary (RoPE) embeddings, speculative decoding etc.