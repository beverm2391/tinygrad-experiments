{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ['DEBUG'] = '4'\n",
    "\n",
    "import time, math\n",
    "from collections import defaultdict\n",
    "from functools import partialmethod, reduce\n",
    "from itertools import accumulate\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Callable, Optional, ClassVar, Type, Union, Sequence, Any, Iterable, Set\n",
    "\n",
    "from tinygrad.helpers import ImageDType, argfix, make_pair, getenv, IMAGE, DEBUG, flatten, DType, dtypes, prod, all_int\n",
    "from tinygrad.lazy import LazyBuffer\n",
    "from tinygrad.ops import Device, LoadOps\n",
    "from tinygrad.shape.symbolic import sint\n",
    "from tinygrad.realize import run_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, device: str, *tensors: Tensor):\n",
    "        self.device = device # device\n",
    "        self.needs_input_grad = [t.requires_grad for t in tensors] # which tensors need grad\n",
    "        self.requires_grad = True if any(self.needs_input_grad) else None if None in self.needs_input_grad else False # if any tensor needs grad\n",
    "        if self.requires_grad: self.parents = tensors  # parents if needed for backprop\n",
    "\n",
    "    def forward(self, *args, **kwargs): raise NotImplementedError\n",
    "    def backward(self, *args, **kwargs): raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(fxn : Type[Function], *x: Tensor, **kwargs) -> Tensor:\n",
    "        ctx = fxn(x[0].device, *x) # construct context\n",
    "        ret = Tensor(ctx.forward(*[t.lazydata for t in x], **kwargs), device=ctx.device, requires_grad=ctx.requires_grad)\n",
    "        if ctx.requires_grad and not Tensor.no_grad: ret._ctx = ctx # used by autograd engine\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    __slots__ = \"lazydata\", \"requires_grad\", \"grad\", \"_ctx\" # this specifically declates which attributes are allowed to save memory\n",
    "    __deletable__ = ('_ctx',) # declare which attributes can be deleted to save memory\n",
    "    training: ClassVar[bool] = True # are we training? i.e. do we need to compute gradients?\n",
    "\n",
    "    # Context manager to enable/disable training (i.e. gradients)\n",
    "    class train:\n",
    "        def __enter__(self):\n",
    "            self.prev = Tensor.training\n",
    "            Tensor.training = True\n",
    "        def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):\n",
    "            Tensor.training = self.prev # revert to previous state\n",
    "\n",
    "    no_grad: ClassVar[bool] = False # always start with gradients enabled\n",
    "    default_type: ClassVar[DType] = dtypes.float32 # default type for new tensors\n",
    "\n",
    "    def __init__(self, data: Union[int, float, list, LazyBuffer, np.ndarray], device: Optional[str] = None, dtype: Optional[DType] = None, requires_grad: Optional[bool] = None):\n",
    "        assert dtype is None or isinstance(dtype, DType), f\"invalid dtype {dtype}\"\n",
    "        device = Device.canonicalize(device) # handles lower stuff\n",
    "        # tensor have gradients, buffers do not\n",
    "        self.grad: Optional[Tensor] = None # gradient\n",
    "\n",
    "        # NOTE: this can be in three states. False and None: no gradient, True: gradient\n",
    "        # None (the default) will be updated to True if it's put in an optimizer\n",
    "        self.requires_grad: Optional[bool] = requires_grad \n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._ctx: Optional[Function] = None # context for autograd\n",
    "\n",
    "        # ! Logic to handle instantiation of different data\n",
    "        if isinstance(data, LazyBuffer):\n",
    "            assert dtype is None or dtype == data.dtype, f\"dtype doesn't match, and casting isn't supported\"\n",
    "        elif isinstance(data, (int, float)): # if we're instantiating from a scalar\n",
    "            data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or Tensor.default_type, device, data)\n",
    "        elif data.__class__ is list:\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesnt have a numpy dtype\"\n",
    "            data = LazyBuffer.fromCPU(np.array(data, dtype=(dtype or Tensor.default_type).np))\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesn't have a numpy dtype\"\n",
    "            if data.shape == (): # if we're instantiating from a scalar\n",
    "                data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or dtypes.from_np(data.dtype), device, data.item())\n",
    "            else: data = LazyBuffer.fromCPU(data.astype(dtype.np) if dtype is not None and dtype.np is not None else data)\n",
    "        else: raise RuntimeError(f\"can't create Tensor from {data}\")\n",
    "\n",
    "        # data is a LazyBuffer, but it might be on the wrong device\n",
    "        self.lazydata = data if data.device == device else data.copy_to_device(device)\n",
    "        #! note the input data is finally stored in the .lazydata attrb as a LazyBuffer\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Tensor {self.lazydata!r} on {self.device} with grad {(self.grad.lazydata if self.grad else None)!r}>\"\n",
    "\n",
    "    # Python has a non moving GC, so this should be okay\n",
    "    def __hash__(self): return id(self)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str: return self.lazydata.device\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> DType: return self.lazydata.dtype\n",
    "\n",
    "    # ! data handlers ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def corealize(lst: Iterable[Tensor]): # realize all tensors in a list\n",
    "        seen: Set[LazyBuffer] = set()\n",
    "        sched = []\n",
    "        for t in lst: sched += t.lazydata.schedule(seen)\n",
    "        run_schedule(sched)\n",
    "\n",
    "    def realize(self) -> Tensor: # realize the tensor (compute all ops)\n",
    "        run_schedule(self.lazydata.schedule())\n",
    "        return self\n",
    "    \n",
    "    def assign(self, x) -> Tensor:\n",
    "        # TODO: this is a hack for writing to DISK\n",
    "        if self.device.startswith(\"DISK\"):\n",
    "            if x.__class__ is not Tensor: x = Tensor(x, device='CPU', dtype=self.dtype) # make tensor\n",
    "            self.contiguous().realize().lazydata.realized_copyin(x.numpy())\n",
    "            return self \n",
    "        if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n",
    "        assert self.shape == x.shape and self.device == self.device, f\"assign shape mismatch {self.shape} != {x.shape} or device mismatch {self.device} != {x.device}\"\n",
    "        assert not x.requires_grad\n",
    "        if DEBUG >= 4: print(f\"assign {self.lazydata} <- {x.lazydata}\")\n",
    "        if self.dtype == x.dtype and self.lazydata.realized is not None and not getenv(\"DISALLOW_ASSIGN\"): x.lazydata.output_buffer = self.lazydata.realized\n",
    "        self.lazydata = x.lazydata\n",
    "        return self\n",
    "    \n",
    "    def detatch(self) -> Tensor: return Tensor(self.lazydata, device=self.device, requires_grad=False)\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        assert all_int(self.shape), f\"no numpy if shape is symbolic, {self.shape=}\"\n",
    "        assert self.dtype.np is not None, f\"no numpy dtype for {self.dtype}\"\n",
    "        return self.detach().cast(dtypes.from_np(self.dtype.np)).contiguous().to('CPU').realize().lazydata.realized.toCPU().reshape(self.shape)\n",
    "\n",
    "    # TODO: if things are realized this won't work\n",
    "    def to_(self, device: str):\n",
    "        assert self.lazydata.realized is None\n",
    "        self.lazydata.device = device\n",
    "        if self.grad: self.grad.to_(device)\n",
    "    \n",
    "    def to(self, device: str) -> Tensor:\n",
    "        ret = Tensor(self.lazydata, device)\n",
    "        if self.grad: ret.grad = self.grad.to(device)\n",
    "        return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
