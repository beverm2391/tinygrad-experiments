{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ['DEBUG'] = '4'\n",
    "\n",
    "import time, math\n",
    "from collections import defaultdict\n",
    "from functools import partialmethod, reduce\n",
    "from itertools import accumulate\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Callable, Optional, ClassVar, Type, Union, Sequence, Any, Iterable, Set\n",
    "\n",
    "from tinygrad.helpers import ImageDType, argfix, make_pair, getenv, IMAGE, DEBUG, flatten, DType, dtypes, prod, all_int\n",
    "from tinygrad.lazy import LazyBuffer\n",
    "from tinygrad.ops import Device, LoadOps\n",
    "from tinygrad.shape.symbolic import sint\n",
    "from tinygrad.realize import run_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, device: str, *tensors: Tensor):\n",
    "        self.device = device # device\n",
    "        self.needs_input_grad = [t.requires_grad for t in tensors] # which tensors need grad\n",
    "        self.requires_grad = True if any(self.needs_input_grad) else None if None in self.needs_input_grad else False # if any tensor needs grad\n",
    "        if self.requires_grad: self.parents = tensors  # parents if needed for backprop\n",
    "\n",
    "    def forward(self, *args, **kwargs): raise NotImplementedError\n",
    "    def backward(self, *args, **kwargs): raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(fxn : Type[Function], *x: Tensor, **kwargs) -> Tensor:\n",
    "        ctx = fxn(x[0].device, *x) # construct context\n",
    "        ret = Tensor(ctx.forward(*[t.lazydata for t in x], **kwargs), device=ctx.device, requires_grad=ctx.requires_grad)\n",
    "        if ctx.requires_grad and not Tensor.no_grad: ret._ctx = ctx # used by autograd engine\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    __slots__ = \"lazydata\", \"requires_grad\", \"grad\", \"_ctx\" # this specifically declates which attributes are allowed to save memory\n",
    "    __deletable__ = ('_ctx',) # declare which attributes can be deleted to save memory\n",
    "    training: ClassVar[bool] = True # are we training? i.e. do we need to compute gradients?\n",
    "\n",
    "    # Context manager to enable/disable training (i.e. gradients)\n",
    "    class train:\n",
    "        def __enter__(self):\n",
    "            self.prev = Tensor.training\n",
    "            Tensor.training = True\n",
    "        def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):\n",
    "            Tensor.training = self.prev # revert to previous state\n",
    "\n",
    "    no_grad: ClassVar[bool] = False # always start with gradients enabled\n",
    "    default_type: ClassVar[DType] = dtypes.float32 # default type for new tensors\n",
    "\n",
    "    def __init__(self, data: Union[int, float, list, LazyBuffer, np.ndarray], device: Optional[str] = None, dtype: Optional[DType] = None, requires_grad: Optional[bool] = None):\n",
    "        assert dtype is None or isinstance(dtype, DType), f\"invalid dtype {dtype}\"\n",
    "        device = Device.canonicalize(device) # handles lower stuff\n",
    "        # tensor have gradients, buffers do not\n",
    "        self.grad: Optional[Tensor] = None # gradient\n",
    "\n",
    "        # NOTE: this can be in three states. False and None: no gradient, True: gradient\n",
    "        # None (the default) will be updated to True if it's put in an optimizer\n",
    "        self.requires_grad: Optional[bool] = requires_grad \n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._ctx: Optional[Function] = None # context for autograd\n",
    "\n",
    "        # ! Logic to handle instantiation of different data\n",
    "        if isinstance(data, LazyBuffer):\n",
    "            assert dtype is None or dtype == data.dtype, f\"dtype doesn't match, and casting isn't supported\"\n",
    "        elif isinstance(data, (int, float)): # if we're instantiating from a scalar\n",
    "            data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or Tensor.default_type, device, data)\n",
    "        elif data.__class__ is list:\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesnt have a numpy dtype\"\n",
    "            data = LazyBuffer.fromCPU(np.array(data, dtype=(dtype or Tensor.default_type).np))\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesn't have a numpy dtype\"\n",
    "            if data.shape == (): # if we're instantiating from a scalar\n",
    "                data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or dtypes.from_np(data.dtype), device, data.item())\n",
    "            else: data = LazyBuffer.fromCPU(data.astype(dtype.np) if dtype is not None and dtype.np is not None else data)\n",
    "        else: raise RuntimeError(f\"can't create Tensor from {data}\")\n",
    "\n",
    "        # data is a LazyBuffer, but it might be on the wrong device\n",
    "        self.lazydata = data if data.device == device else data.copy_to_device(device)\n",
    "        #! note the input data is finally stored in the .lazydata attrb as a LazyBuffer\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Tensor {self.lazydata!r} on {self.device} with grad {(self.grad.lazydata if self.grad else None)!r}>\"\n",
    "\n",
    "    # Python has a non moving GC, so this should be okay\n",
    "    def __hash__(self): return id(self)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str: return self.lazydata.device\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> DType: return self.lazydata.dtype\n",
    "\n",
    "    # ! data handlers ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def corealize(lst: Iterable[Tensor]): # realize all tensors in a list\n",
    "        seen: Set[LazyBuffer] = set()\n",
    "        sched = []\n",
    "        for t in lst: sched += t.lazydata.schedule(seen)\n",
    "        run_schedule(sched)\n",
    "\n",
    "    def realize(self) -> Tensor: # realize the tensor (compute all ops)\n",
    "        run_schedule(self.lazydata.schedule())\n",
    "        return self\n",
    "    \n",
    "    def assign(self, x) -> Tensor:\n",
    "        # TODO: this is a hack for writing to DISK\n",
    "        if self.device.startswith(\"DISK\"):\n",
    "            if x.__class__ is not Tensor: x = Tensor(x, device='CPU', dtype=self.dtype) # make tensor\n",
    "            self.contiguous().realize().lazydata.realized_copyin(x.numpy())\n",
    "            return self \n",
    "        if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n",
    "        assert self.shape == x.shape and self.device == self.device, f\"assign shape mismatch {self.shape} != {x.shape} or device mismatch {self.device} != {x.device}\"\n",
    "        assert not x.requires_grad\n",
    "        if DEBUG >= 4: print(f\"assign {self.lazydata} <- {x.lazydata}\")\n",
    "        if self.dtype == x.dtype and self.lazydata.realized is not None and not getenv(\"DISALLOW_ASSIGN\"): x.lazydata.output_buffer = self.lazydata.realized\n",
    "        self.lazydata = x.lazydata\n",
    "        return self\n",
    "    \n",
    "    def detatch(self) -> Tensor: return Tensor(self.lazydata, device=self.device, requires_grad=False)\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        assert all_int(self.shape), f\"no numpy if shape is symbolic, {self.shape=}\"\n",
    "        assert self.dtype.np is not None, f\"no numpy dtype for {self.dtype}\"\n",
    "        return self.detach().cast(dtypes.from_np(self.dtype.np)).contiguous().to('CPU').realize().lazydata.realized.toCPU().reshape(self.shape)\n",
    "\n",
    "    # TODO: if things are realized this won't work\n",
    "    def to_(self, device: str):\n",
    "        assert self.lazydata.realized is None\n",
    "        self.lazydata.device = device\n",
    "        if self.grad: self.grad.to_(device)\n",
    "    \n",
    "    def to(self, device: str) -> Tensor:\n",
    "        ret = Tensor(self.lazydata, device)\n",
    "        if self.grad: ret.grad = self.grad.to(device)\n",
    "        return ret\n",
    "    \n",
    "    #! Creation llop entrypoint ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def _loadop(op, sz, device: Optional[str] = None, dtype: Optional[DType] = None, arg = None, **kwargs):\n",
    "        return Tensor(LazyBuffer.loadop(op, (sz,), Tensor.default_type if dtype is None else dtype, Device.canonicalize(device), arg), dtype=dtype, device=device, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty(*shape, **kwargs):\n",
    "        assert all_int(shape), f\"cannot create with symbolic shape {shape}\"\n",
    "        return Tensor._loadop(LoadOps.EMPTY, prod(shape), **kwargs).reshape(shape)\n",
    "    \n",
    "    _seed: int = int(time.time()) # interesting, using time for seed (ClassAtr)\n",
    "    @staticmethod\n",
    "    def manual_seed(seed=0): Tensor._seed = seed # set seed\n",
    "\n",
    "    @staticmethod\n",
    "    def rand(*shape, **kwargs):\n",
    "        assert all_int(shape), f\"cannot create with symbolic shape {shape}\"\n",
    "        Tensor._seed += 1\n",
    "        return Tensor._loadop(LoadOps.RAND, prod(shape), arg=Tensor._seed, **kwargs).reshape(shape)\n",
    "    \n",
    "    # ! Creation helper functions ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def full(shape: Tuple[sint, ...], fill_value, **kwargs): return Tensor(fill_value, **kwargs).reshape([1]*len(new_shape := argfix(shape))).expand(new_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(*shape, **kwargs): return Tensor.full(argfix(*shape), 0, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(*shape, **kwargs): return Tensor.full(argfix(*shape), 1, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def arrange(start, stop=None, step=1, **kwargs):\n",
    "        if stop is None: stop, start = start, 0\n",
    "        return Tensor.full(math.ceil((stop-start)/step), start, **kwargs).cumsum() + (start - step)\n",
    "    \n",
    "    @staticmethod\n",
    "    def eye(dim: int, **kwargs): return Tensor.full((dim,1),1,**kwargs).pad(((0,0),(0,dim))).reshape(dim*(dim+1)).shrink(((0,dim*dim),)).reshape(dim, dim)\n",
    "\n",
    "    def full_like(self, fill_value, **kwargs):\n",
    "        return Tensor.full(self.shape, fill_value=fill_value, dtype=kwargs.pop(\"dtype\", self.dtype), device=kwargs.pop(\"device\", self.device), **kwargs)\n",
    "    def zeros_like(self, **kwargs): return self.full_like(0, **kwargs)\n",
    "    def ones_like(self, **kwargs): return self.full_like(1, **kwargs)\n",
    "\n",
    "    # ! rng hlops ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def randn(*shape, dtype: Optional[DType] = None, **kwargs) -> Tensor:\n",
    "        # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform\n",
    "        src = Tensor.rand(2, *shape, **kwargs)\n",
    "        return src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(Tensor.default_type if dtype is None else dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal(*shape, mean=0.0, std=1.0, **kwargs) -> Tensor: return (std * Tensor.randn(*shape, **kwargs)) + mean\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(*shape, low=-1.0, high=1.0, **kwargs) -> Tensor:\n",
    "        dtype = kwargs.pop(\"dtype\", Tensor.default_type)\n",
    "        return ((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype) + low\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_uniform(*shape, **kwargs) -> Tensor: return Tensor.uniform(*shape, **kwargs).mul(prod(shape)**-0.5)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform\n",
    "    @staticmethod\n",
    "    def glorot_uniform(*shape, **kwargs) -> Tensor: return Tensor.uniform(*shape, **kwargs).mul((6/(shape[0]+prod(shape[1:])))**0.5)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_\n",
    "    @staticmethod\n",
    "    def kaiming_uniform(*shape, a:float = 0.01, **kwargs) -> Tensor:\n",
    "        bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(shape[1:]))\n",
    "        return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_\n",
    "    @staticmethod\n",
    "    def kaiming_normal(*shape, a:float = 0.01, **kwargs) -> Tensor:\n",
    "        std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(shape[1:]))\n",
    "        return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)\n",
    "    \n",
    "    # ! toposort and backward pass ====================\n",
    "    def deepwalk(self):\n",
    "        def _deepwalk(node, visited, nodes):\n",
    "            visited.add(node)\n",
    "            if getattr(node, \"_ctx\", None):\n",
    "                for i in node._ctx.parents:\n",
    "                    if i not in visited: _deepwalk(i, visited, nodes)\n",
    "            nodes.append(node)\n",
    "            return nodes\n",
    "        return _deepwalk(self, set(), [])\n",
    "    \n",
    "    def backward(self):\n",
    "        assert self.shape == tuple(), f\"backward can only be called for scalar tensor, but it has shape {self.shape}\"\n",
    "\n",
    "        # fill in the first grad with one. don't use Tensor.ones because we don't need contiguous\n",
    "        # this is \"implicit gradient creation\"\n",
    "        self.grad = Tensor(1, device=self.device, requires_grad=False)\n",
    "\n",
    "        # walk the graph in reverse\n",
    "        for t0 in reversed(self.deepwalk()):\n",
    "            assert (t0.grad is not None)\n",
    "            grads = t0._ctx.backward(t0.grad.lazydata)\n",
    "            grads = [Tensor(g, device=self.device, requires_grad=False) if g is not None else None\n",
    "                        for g in ([grads] if len(t0._ctx.parents) == 1 else grads)]\n",
    "            for t, g in zip(t0._ctx.parents, grads):\n",
    "                if g is not None and t.requires_grad:\n",
    "                    assert g.shape == t.shape, f\"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}\"\n",
    "                    t.grad = g if t.grad is None else (t.grad + g)\n",
    "                del t0._ctx\n",
    "\n",
    "    #  ! movement mlops ===================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
