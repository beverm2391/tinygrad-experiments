{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ['DEBUG'] = '4'\n",
    "\n",
    "import time, math\n",
    "from collections import defaultdict\n",
    "from functools import partialmethod, reduce\n",
    "from itertools import accumulate\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Callable, Optional, ClassVar, Type, Union, Sequence, Any, Iterable, Set\n",
    "\n",
    "from tinygrad.helpers import ImageDType, argfix, make_pair, getenv, IMAGE, DEBUG, flatten, DType, dtypes, prod, all_int\n",
    "from tinygrad.lazy import LazyBuffer\n",
    "from tinygrad.ops import Device, LoadOps\n",
    "from tinygrad.shape.symbolic import sint\n",
    "from tinygrad.realize import run_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, device: str, *tensors: Tensor):\n",
    "        self.device = device # device\n",
    "        self.needs_input_grad = [t.requires_grad for t in tensors] # which tensors need grad\n",
    "        self.requires_grad = True if any(self.needs_input_grad) else None if None in self.needs_input_grad else False # if any tensor needs grad\n",
    "        if self.requires_grad: self.parents = tensors  # parents if needed for backprop\n",
    "\n",
    "    def forward(self, *args, **kwargs): raise NotImplementedError\n",
    "    def backward(self, *args, **kwargs): raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def apply(fxn : Type[Function], *x: Tensor, **kwargs) -> Tensor:\n",
    "        ctx = fxn(x[0].device, *x) # construct context\n",
    "        ret = Tensor(ctx.forward(*[t.lazydata for t in x], **kwargs), device=ctx.device, requires_grad=ctx.requires_grad)\n",
    "        if ctx.requires_grad and not Tensor.no_grad: ret._ctx = ctx # used by autograd engine\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad.mlops as mlops\n",
    "\n",
    "class Tensor:\n",
    "    __slots__ = \"lazydata\", \"requires_grad\", \"grad\", \"_ctx\" # this specifically declates which attributes are allowed to save memory\n",
    "    __deletable__ = ('_ctx',) # declare which attributes can be deleted to save memory\n",
    "    training: ClassVar[bool] = True # are we training? i.e. do we need to compute gradients?\n",
    "\n",
    "    # Context manager to enable/disable training (i.e. gradients)\n",
    "    class train:\n",
    "        def __enter__(self):\n",
    "            self.prev = Tensor.training\n",
    "            Tensor.training = True\n",
    "        def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):\n",
    "            Tensor.training = self.prev # revert to previous state\n",
    "\n",
    "    no_grad: ClassVar[bool] = False # always start with gradients enabled\n",
    "    default_type: ClassVar[DType] = dtypes.float32 # default type for new tensors\n",
    "\n",
    "    def __init__(self, data: Union[int, float, list, LazyBuffer, np.ndarray], device: Optional[str] = None, dtype: Optional[DType] = None, requires_grad: Optional[bool] = None):\n",
    "        assert dtype is None or isinstance(dtype, DType), f\"invalid dtype {dtype}\"\n",
    "        device = Device.canonicalize(device) # handles lower stuff\n",
    "        # tensor have gradients, buffers do not\n",
    "        self.grad: Optional[Tensor] = None # gradient\n",
    "\n",
    "        # NOTE: this can be in three states. False and None: no gradient, True: gradient\n",
    "        # None (the default) will be updated to True if it's put in an optimizer\n",
    "        self.requires_grad: Optional[bool] = requires_grad \n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._ctx: Optional[Function] = None # context for autograd\n",
    "\n",
    "        # ! Logic to handle instantiation of different data\n",
    "        if isinstance(data, LazyBuffer):\n",
    "            assert dtype is None or dtype == data.dtype, f\"dtype doesn't match, and casting isn't supported\"\n",
    "        elif isinstance(data, (int, float)): # if we're instantiating from a scalar\n",
    "            data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or Tensor.default_type, device, data)\n",
    "        elif data.__class__ is list:\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesnt have a numpy dtype\"\n",
    "            data = LazyBuffer.fromCPU(np.array(data, dtype=(dtype or Tensor.default_type).np))\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            assert dtype is None or dtype.np is not None, f\"{dtype} doesn't have a numpy dtype\"\n",
    "            if data.shape == (): # if we're instantiating from a scalar\n",
    "                data = LazyBuffer.loadop(LoadOps.CONST, tuple(), dtype or dtypes.from_np(data.dtype), device, data.item())\n",
    "            else: data = LazyBuffer.fromCPU(data.astype(dtype.np) if dtype is not None and dtype.np is not None else data)\n",
    "        else: raise RuntimeError(f\"can't create Tensor from {data}\")\n",
    "\n",
    "        # data is a LazyBuffer, but it might be on the wrong device\n",
    "        self.lazydata = data if data.device == device else data.copy_to_device(device)\n",
    "        #! note the input data is finally stored in the .lazydata attrb as a LazyBuffer\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Tensor {self.lazydata!r} on {self.device} with grad {(self.grad.lazydata if self.grad else None)!r}>\"\n",
    "\n",
    "    # Python has a non moving GC, so this should be okay\n",
    "    def __hash__(self): return id(self)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str: return self.lazydata.device\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> DType: return self.lazydata.dtype\n",
    "\n",
    "    # ! data handlers ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def corealize(lst: Iterable[Tensor]): # realize all tensors in a list\n",
    "        seen: Set[LazyBuffer] = set()\n",
    "        sched = []\n",
    "        for t in lst: sched += t.lazydata.schedule(seen)\n",
    "        run_schedule(sched)\n",
    "\n",
    "    def realize(self) -> Tensor: # realize the tensor (compute all ops)\n",
    "        run_schedule(self.lazydata.schedule())\n",
    "        return self\n",
    "    \n",
    "    def assign(self, x) -> Tensor:\n",
    "        # TODO: this is a hack for writing to DISK\n",
    "        if self.device.startswith(\"DISK\"):\n",
    "            if x.__class__ is not Tensor: x = Tensor(x, device='CPU', dtype=self.dtype) # make tensor\n",
    "            self.contiguous().realize().lazydata.realized_copyin(x.numpy())\n",
    "            return self \n",
    "        if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n",
    "        assert self.shape == x.shape and self.device == self.device, f\"assign shape mismatch {self.shape} != {x.shape} or device mismatch {self.device} != {x.device}\"\n",
    "        assert not x.requires_grad\n",
    "        if DEBUG >= 4: print(f\"assign {self.lazydata} <- {x.lazydata}\")\n",
    "        if self.dtype == x.dtype and self.lazydata.realized is not None and not getenv(\"DISALLOW_ASSIGN\"): x.lazydata.output_buffer = self.lazydata.realized\n",
    "        self.lazydata = x.lazydata\n",
    "        return self\n",
    "    \n",
    "    def detatch(self) -> Tensor: return Tensor(self.lazydata, device=self.device, requires_grad=False)\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        assert all_int(self.shape), f\"no numpy if shape is symbolic, {self.shape=}\"\n",
    "        assert self.dtype.np is not None, f\"no numpy dtype for {self.dtype}\"\n",
    "        return self.detach().cast(dtypes.from_np(self.dtype.np)).contiguous().to('CPU').realize().lazydata.realized.toCPU().reshape(self.shape)\n",
    "\n",
    "    # TODO: if things are realized this won't work\n",
    "    def to_(self, device: str):\n",
    "        assert self.lazydata.realized is None\n",
    "        self.lazydata.device = device\n",
    "        if self.grad: self.grad.to_(device)\n",
    "    \n",
    "    def to(self, device: str) -> Tensor:\n",
    "        ret = Tensor(self.lazydata, device)\n",
    "        if self.grad: ret.grad = self.grad.to(device)\n",
    "        return ret\n",
    "    \n",
    "    #! Creation llop entrypoint ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def _loadop(op, sz, device: Optional[str] = None, dtype: Optional[DType] = None, arg = None, **kwargs):\n",
    "        return Tensor(LazyBuffer.loadop(op, (sz,), Tensor.default_type if dtype is None else dtype, Device.canonicalize(device), arg), dtype=dtype, device=device, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty(*shape, **kwargs):\n",
    "        assert all_int(shape), f\"cannot create with symbolic shape {shape}\"\n",
    "        return Tensor._loadop(LoadOps.EMPTY, prod(shape), **kwargs).reshape(shape)\n",
    "    \n",
    "    _seed: int = int(time.time()) # interesting, using time for seed (ClassAtr)\n",
    "    @staticmethod\n",
    "    def manual_seed(seed=0): Tensor._seed = seed # set seed\n",
    "\n",
    "    @staticmethod\n",
    "    def rand(*shape, **kwargs):\n",
    "        assert all_int(shape), f\"cannot create with symbolic shape {shape}\"\n",
    "        Tensor._seed += 1\n",
    "        return Tensor._loadop(LoadOps.RAND, prod(shape), arg=Tensor._seed, **kwargs).reshape(shape)\n",
    "    \n",
    "    # ! Creation helper functions ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def full(shape: Tuple[sint, ...], fill_value, **kwargs): return Tensor(fill_value, **kwargs).reshape([1]*len(new_shape := argfix(shape))).expand(new_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(*shape, **kwargs): return Tensor.full(argfix(*shape), 0, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(*shape, **kwargs): return Tensor.full(argfix(*shape), 1, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def arrange(start, stop=None, step=1, **kwargs):\n",
    "        if stop is None: stop, start = start, 0\n",
    "        return Tensor.full(math.ceil((stop-start)/step), start, **kwargs).cumsum() + (start - step)\n",
    "    \n",
    "    @staticmethod\n",
    "    def eye(dim: int, **kwargs): return Tensor.full((dim,1),1,**kwargs).pad(((0,0),(0,dim))).reshape(dim*(dim+1)).shrink(((0,dim*dim),)).reshape(dim, dim)\n",
    "\n",
    "    def full_like(self, fill_value, **kwargs):\n",
    "        return Tensor.full(self.shape, fill_value=fill_value, dtype=kwargs.pop(\"dtype\", self.dtype), device=kwargs.pop(\"device\", self.device), **kwargs)\n",
    "    def zeros_like(self, **kwargs): return self.full_like(0, **kwargs)\n",
    "    def ones_like(self, **kwargs): return self.full_like(1, **kwargs)\n",
    "\n",
    "    # ! rng hlops ====================\n",
    "\n",
    "    @staticmethod\n",
    "    def randn(*shape, dtype: Optional[DType] = None, **kwargs) -> Tensor:\n",
    "        # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform\n",
    "        src = Tensor.rand(2, *shape, **kwargs)\n",
    "        return src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(Tensor.default_type if dtype is None else dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal(*shape, mean=0.0, std=1.0, **kwargs) -> Tensor: return (std * Tensor.randn(*shape, **kwargs)) + mean\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(*shape, low=-1.0, high=1.0, **kwargs) -> Tensor:\n",
    "        dtype = kwargs.pop(\"dtype\", Tensor.default_type)\n",
    "        return ((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype) + low\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_uniform(*shape, **kwargs) -> Tensor: return Tensor.uniform(*shape, **kwargs).mul(prod(shape)**-0.5)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform\n",
    "    @staticmethod\n",
    "    def glorot_uniform(*shape, **kwargs) -> Tensor: return Tensor.uniform(*shape, **kwargs).mul((6/(shape[0]+prod(shape[1:])))**0.5)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_\n",
    "    @staticmethod\n",
    "    def kaiming_uniform(*shape, a:float = 0.01, **kwargs) -> Tensor:\n",
    "        bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(shape[1:]))\n",
    "        return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_\n",
    "    @staticmethod\n",
    "    def kaiming_normal(*shape, a:float = 0.01, **kwargs) -> Tensor:\n",
    "        std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(shape[1:]))\n",
    "        return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)\n",
    "    \n",
    "    # ! toposort and backward pass ====================\n",
    "    def deepwalk(self):\n",
    "        def _deepwalk(node, visited, nodes):\n",
    "            visited.add(node)\n",
    "            if getattr(node, \"_ctx\", None):\n",
    "                for i in node._ctx.parents:\n",
    "                    if i not in visited: _deepwalk(i, visited, nodes)\n",
    "            nodes.append(node)\n",
    "            return nodes\n",
    "        return _deepwalk(self, set(), [])\n",
    "    \n",
    "    def backward(self):\n",
    "        assert self.shape == tuple(), f\"backward can only be called for scalar tensor, but it has shape {self.shape}\"\n",
    "\n",
    "        # fill in the first grad with one. don't use Tensor.ones because we don't need contiguous\n",
    "        # this is \"implicit gradient creation\"\n",
    "        self.grad = Tensor(1, device=self.device, requires_grad=False)\n",
    "\n",
    "        # walk the graph in reverse\n",
    "        for t0 in reversed(self.deepwalk()):\n",
    "            assert (t0.grad is not None)\n",
    "            grads = t0._ctx.backward(t0.grad.lazydata)\n",
    "            grads = [Tensor(g, device=self.device, requires_grad=False) if g is not None else None\n",
    "                        for g in ([grads] if len(t0._ctx.parents) == 1 else grads)]\n",
    "            for t, g in zip(t0._ctx.parents, grads):\n",
    "                if g is not None and t.requires_grad:\n",
    "                    assert g.shape == t.shape, f\"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}\"\n",
    "                    t.grad = g if t.grad is None else (t.grad + g)\n",
    "                del t0._ctx\n",
    "\n",
    "    #  ! movement mlops ====================\n",
    "    def reshape(self, shape, *args) -> Tensor:\n",
    "        new_shape = argfix(shape, *args)  # Standardize shape argument with argfix\n",
    "        assert 0 not in new_shape, f\"zeros not allowed in shape {new_shape}\" # Ensure no zero dimensions in new shape\n",
    "        # If dimension is -1, auto-calculate it to maintain total element count\n",
    "        return mlops.Reshape.apply(self, shape=tuple([-prod(self.shape) // prod(new_shape) if s == -1 else s  for s in new_shape]))\n",
    "\n",
    "    def expand(self, shape, *args) -> Tensor:\n",
    "        # Use argfix to standardize the shape, then expand dimensions\n",
    "        # If dimension is -1, keep the original size in that dimension\n",
    "        return mlops.Expand.apply(self, shape=tuple([x if x != -1 else s for s, x in zip(self.shape, argfix(shape, *args))]))\n",
    "\n",
    "    def permute(self, order, *args) -> Tensor:\n",
    "        # Permute dimensions based on the order given, standardized by argfix\n",
    "        return mlops.Permute.apply(self, order=argfix(order, *args))\n",
    "\n",
    "    def flip(self, axis, *args) -> Tensor:\n",
    "        # Flip the tensor along specified axes.\n",
    "        # Negative axis values are adjusted to positive by adding len(self.shape)\n",
    "        return mlops.Flip.apply(self, axis=[x if x >= 0 else x + len(self.shape) for x in argfix(axis, *args)])\n",
    "\n",
    "    def shrink(self, arg: Tuple[Tuple[sint, sint], ...]) -> Tensor:\n",
    "        # Shrink tensor only if the shrinking arg is different from the tensor's shape\n",
    "        return mlops.Shrink.apply(self, arg=arg) if any(x != (0, s) for x, s in zip(arg, self.shape)) else self\n",
    "\n",
    "    def pad(self, arg: Tuple[Tuple[int, int], ...], value: float = 0) -> Tensor:\n",
    "        # Apply padding to tensor; if arg is all zeros, return the original tensor\n",
    "        ret = mlops.Pad.apply(self, arg=arg) if any(x != (0, 0) for x in arg) else self\n",
    "        # If value is non-zero, add it to the padding\n",
    "        return ret if 0 == value else ret + mlops.Pad.apply(Tensor.ones_like(self), arg=arg).where(0, value)\n",
    "\n",
    "    # ***** movement hlops *****\n",
    "\n",
    "    # - Negative indices are taken relative to the end of the sequence, so X[-2] returns the 2nd-to-last element\n",
    "    # - A slice i:j returns the elements with indices in [i, j)\n",
    "    #    - If omitted, i and j will default to 0 and N, respectively, where N is the length of the sequence\n",
    "    #    - Negative values for i and j are taken relative to the end of the sequence\n",
    "    #    - Both i and j will be clamped to the range (-N, N], where N in the length of the sequence\n",
    "    # - Indexing with None on a given axis will add a new dimension of size one before that axis\n",
    "    # - Empty slices are not allowed (tensors with 0s in shape have to be supported first, for all backends).\n",
    "    # - For a slice [i:j:k] finding the correct indices is delegated to slice.indices(len).\n",
    "    # - Strides > 1 and < 0 are now allowed!:\n",
    "    #    - This works by applying Shrink -> [[Flip -> ] Pad -> Reshape -> Shrink] -> Reshape (ops in brackets are optional)\n",
    "    #    - Idea of stride < 0 support:\n",
    "    #        - Do the slice first, flip the axes were slice.step is negative, do slice.step -> -slice.step. Go to steps below.\n",
    "    #    - Idea of stride `s` > 1 support (Pad -> Reshape -> Shrink):\n",
    "    #        - Instead of doing [::s] on axis [dim_sz], do [:, 0] on axes [dim_sz_padded // s, s].\n",
    "    #        - So pad dim_sz with as many zeros as needed (dim_sz -> dim_sz_padded) so that reshape to [dim_sz_padded // s, s]\n",
    "    #          is possible.\n",
    "    #        - Apply Shrink to do the slice [:, 0] on axes of shapes [dim_sz_padded // s, s].\n",
    "    # - Fancy indexing and combined indexing is supported\n",
    "    #    - Combined indexing works by letting regular slicing finish first -> computing the resulting dims w.r.t to Tensors passed in -> fancy indexing\n",
    "    #    - Any Tensors passed in __getitem__ will perform (CMPEQ with arange -> MUL with self -> SUM_REDUCE) iteratively\n",
    "    #        - The first iteration will expand the dim of self while consecutive iterations will reduce the dim\n",
    "    #    - There's a special case where a permute is needed at the end:\n",
    "    #        - if first Tensor passed in (expand dims) is not at dim 0\n",
    "    #        - and following Tensors does not follow consecutively to the end of fancy indexing's dims\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        def normalize_int(e, i, dim_sz): # handle bounds\n",
    "            if -dim_sz <= e < dim_sz: return e if e != -1 else dim_sz-1\n",
    "            raise IndexError(f\"index {e} is out of bounds for dimension {i} with size {self.shape[i]}\")\n",
    "        \n",
    "        orig_slices = list(val) if isinstance(val, tuple) else [val] # store the original slices arg\n",
    "        count = defaultdict(list) \n",
    "        for i, v in enumerate(orig_slices):\n",
    "            count[type(v)].append(i) # count the number of each type of slice\n",
    "\n",
    "        # check for too many slices\n",
    "        if (num_slices := len(count[int]) + len(count[slice]) + len(count[Tensor])) > len(self.shape):\n",
    "            raise IndexError(f\"too many indices for tensor of dimension {len(self.shape)}\")\n",
    "        \n",
    "        # check for too many ellipses\n",
    "        if len(ellipsis_found := count[type(Ellipsis)]) > 1:\n",
    "            raise IndexError(f\"an index can only have a single ellipsis ('...')\")\n",
    "        \n",
    "        ellipsis_idx = ellipsis_found[0] if ellipsis_found else len(orig_slices) # find the index of the ellipsis\n",
    "        orig_slices[ellipsis_idx:ellipsis_idx+1] = [slice(None)] * (len(self.shape) - num_slices + 1) # replace the ellipsis with slices\n",
    "\n",
    "        # Create a list of valid slices to apply to the tensor.\n",
    "        # - Use slice directly if it is a slice object.\n",
    "        # - Convert integer indices to slices after normalizing them.\n",
    "        # - Use a full slice (slice(None)) for other types (like Tensor).\n",
    "        valid_slices = [\n",
    "            v if isinstance(v, slice)  # Keep slice as-is\n",
    "            else slice(y_ := normalize_int(v, i, dim_sz), y_ + 1)  # Convert int to slice\n",
    "            if isinstance(v, int) \n",
    "            else slice(None)  # For other types, use a full slice\n",
    "            for i, (v, dim_sz) in enumerate(zip(valid_slices, self.shape))  # Iterate over dimensions\n",
    "        ]\n",
    "\n",
    "        # Unpack indices into start, stop, strides based on slices and shape; set to empty if no valid slices\n",
    "        start, stop, strides = zip(*y) if (y := [s.indices(dim_sz) for s, dim_sz in zip(valid_slices, self.shape)]) else ((), (), ())\n",
    "        # Adjust start and end for each slice based on stride direction\n",
    "        new_slice = tuple((s, e) if st > 0 else (e + 1, s + 1) for s, e, st in zip(start, stop, strides))\n",
    "        # Apply shrink and flip operations based on calculated slice; flip on negative strides\n",
    "        sliced_tensor = self.shrink(new_slice).flip(axis=[i for i, s in enumerate(strides) if s < 0])\n",
    "        new_shape = sliced_tensor.shape  # Get the shape of the newly sliced tensor\n",
    "\n",
    "        if any(abs(s) != 1 for s in strides): # Check if any stride is not equal to 1 (in absolute value)\n",
    "            strides = tuple(abs(s) != 1 for s in strides) # Store which strides are not 1 as a tuple of booleans\n",
    "            # Pad the tensor to match new strides; if dimension size % stride != 0, padding needed\n",
    "            padded_tensor = sliced_tensor.pad(tuple((0, s - (dim_sz % s) if dim_sz % s != 0 else 0) for s, dim_sz in zip(strides, sliced_tensor.shape)))\n",
    "            # Reshape tensor to accommodate strides, splitting dimensions\n",
    "            reshaped_tensor = padded_tensor.reshape(flatten([sh // s, s] for sh, s in zip(padded_tensor.shape, strides)))\n",
    "            new_shape = reshaped_tensor.shape[::2] # Get the shape for dimensions affected by the non-1 strides\n",
    "            sliced_tensor = reshaped_tensor.shrink(tuple(flatten(((0, sh), (0, 1)) for sh in new_shape))) # Shrink tensor to finalize operation, effectively applying the strides\n",
    "\n",
    "        final_shape, it_shape, dim, tensors, dim_collapsed = [], iter(new_shape), [], [], 0\n",
    "        # Iterate over original slices to compute the final shape and other parameters\n",
    "        for i, s in enumerate(orig_slices):\n",
    "            if s is None:\n",
    "                final_shape.append(1)  # Add a dimension of size 1 if slice is None\n",
    "            else:  # s can be int, slice, or Tensor\n",
    "                dim_shape = next(it_shape)  # Fetch next shape dimension\n",
    "                if isinstance(s, int): dim_collapsed += 1  # Count how many dimensions are collapsed\n",
    "                else:\n",
    "                    assert isinstance(dim_shape, int), f\"does not support symbolic shape {dim_shape}\"\n",
    "                    final_shape.append(dim_shape)  # Add to final shape\n",
    "                    if isinstance(s, Tensor): # If slice is a Tensor, store it and its adjusted dimension\n",
    "                        tensors.append(s)\n",
    "                        dim.append(i - dim_collapsed)\n",
    "\n",
    "        # Reshape the sliced tensor to its final shape\n",
    "        ret = sliced_tensor.reshape(tuple(final_shape))\n",
    "\n",
    "        if tensors: # Fancy/tensor indexing\n",
    "            # normalize idx\n",
    "            # TODO: first contiguous fixes torch+cpu_only CI, but it causes llvm to fail. Second one fixes llvm\n",
    "            idx = [t.sign().contiguous().__neg__().contiguous().relu() * ret.shape[d] + t for d,t in zip(dim, tensors)]\n",
    "            max_dim = max(i.ndim for i in idx)\n",
    "            # compute sum_dim, arange, and idx\n",
    "            sum_dim = [d if n==0 else d+max_dim-n for n,d in enumerate(dim)]\n",
    "            arange = [Tensor.arange(ret.shape[d], dtype=dtypes.int32, requires_grad=False, device=self.device).reshape(*[1]*sd, ret.shape[d], *[1]*(ret.ndim + max_dim - n - sd - 1)) for n,(sd,d) in enumerate(zip(sum_dim, dim))]\n",
    "            first_idx = [idx[0].reshape(*[1]*dim[0], *[1]*(1 + max_dim - idx[0].ndim), *idx[0].shape, *[1]*(ret.ndim - dim[0] - 1))]\n",
    "            rest_idx = [i.reshape(*[1]*dim[0], *[1]*(max_dim - i.ndim), *i.shape, *[1]*(ret.ndim - dim[0] - n)) for n,i in enumerate(idx[1:], 1)]\n",
    "            idx = first_idx + rest_idx\n",
    "            ret = ret.reshape(*ret.shape[:sum_dim[0]+1], *[1]*max_dim, *ret.shape[sum_dim[0]+1:])\n",
    "            # iteratively fancy index\n",
    "            for a,i,sd in zip(arange, idx, sum_dim): ret = (a==i).mul(ret).sum(sd)\n",
    "            # special permute case\n",
    "            if dim[0] != 0 and len(dim) != 1 and dim != list(range(dim[0], dim[-1]+1)):\n",
    "                ret_dims = list(range(ret.ndim))\n",
    "                ret = ret.permute(ret_dims[dim[0]:dim[0]+max_dim] + ret_dims[:dim[0]] + ret_dims[dim[0]+max_dim:])\n",
    "            return ret\n",
    "        \n",
    "    def __setitem__(self,s,v): return self.__getitem__(s).assign(v)\n",
    "\n",
    "    # NOTE: using slice is discouraged and things should migrate to pad and shrink\n",
    "    def slice(self, arg:Sequence[Optional[Tuple[int, sint]]], value:float=0) -> Tensor:\n",
    "        arg_ = tuple([a if a is not None else (0,s) for s,a in zip(self.shape, arg)])\n",
    "        padding = tuple([(max(0, -p[0]), max(0, p[1]-self.shape[i])) for i,p in enumerate(arg_)])\n",
    "        return self.pad(padding, value=value).shrink(tuple([(p[0] + padding[i][0], p[1] + padding[i][0]) for i,p in enumerate(arg_)]))\n",
    "    \n",
    "    def gather(self: Tensor, idx: Tensor, dim: int):\n",
    "        assert idx.ndim == self.ndim, \"self.ndim must equal idx.ndim\"\n",
    "        assert all(s >= i for s,i in zip(self.shape, idx.shape)), \"all dim of idx.shape must be smaller than self.shape\"\n",
    "        if dim < 0: dim += self.ndim\n",
    "        idx = idx.transpose(ax1=dim, ax2=0).unsqueeze(-1)\n",
    "        permarg = list(range(self.ndim))\n",
    "        permarg = permarg[1:dim] + [permarg[0]] + permarg[dim+1:] + [permarg[dim]] if dim != 0 else permarg[1:] + [permarg[0]]\n",
    "        return ((idx == Tensor.arange(self.shape[dim], dtype=dtypes.int32, requires_grad=False, device=self.device)) * self.permute(*permarg).shrink(tuple([*[(0,sh) for sh in idx.shape[1:-1]], (0,self.shape[dim])])).unsqueeze(0)).sum(-1).transpose(ax1=0, ax2=dim)\n",
    "\n",
    "    def cat(self, *args, dim=0):\n",
    "        dim = (dim + len(self.shape)) if dim < 0 else dim\n",
    "        assert all(len(y.shape) == len(self.shape) and all(y.shape[i] == s for i,s in enumerate(self.shape) if i != dim) for y in args)\n",
    "        catargs = [self, *args]\n",
    "        assert all(t.shape for t in catargs), \"zero-dimensional tensor cannot be concatenated\"\n",
    "        shapes = [s.shape[dim] for s in catargs]\n",
    "        shape_cumsum = [0, *accumulate(shapes)]\n",
    "        slc = [[(0, 0) for _ in self.shape] for _ in catargs]\n",
    "        for shp,k,s in zip(shapes, shape_cumsum[:-1], slc):\n",
    "            s[dim] = (k, shape_cumsum[-1] - k - shp)\n",
    "        return reduce(Tensor.__add__, [arg.pad(tuple(s)) for arg,s in zip(catargs, slc)])\n",
    "\n",
    "    @staticmethod\n",
    "    def stack(tensors, dim=0):\n",
    "        first = tensors[0].unsqueeze(dim)\n",
    "        unsqueezed_tensors = [tensor.unsqueeze(dim) for tensor in tensors[1:]]\n",
    "        # checks for shapes and number of dimensions delegated to cat\n",
    "        return first.cat(*unsqueezed_tensors, dim=dim)\n",
    "\n",
    "    def repeat(self, repeats):\n",
    "        base_shape = (1,) * (len(repeats) - self.ndim) + self.shape\n",
    "        new_shape = [x for b in base_shape for x in [1, b]]\n",
    "        expand_shape = [x for rs in zip(repeats, base_shape) for x in rs]\n",
    "        final_shape = [r*s for r,s in zip(repeats, base_shape)]\n",
    "        return self.reshape(new_shape).expand(expand_shape).reshape(final_shape)\n",
    "\n",
    "    def chunk(self, num:int, dim:int) -> List[Tensor]:\n",
    "        assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n",
    "        dim, step = dim + self.ndim if dim < 0 else dim, math.ceil(self.shape[dim]/num)\n",
    "        slice_params = [[slice(None)]*dim + [slice(k, k + step)] for k in range(0, self.shape[dim], step)]\n",
    "        return [self[tuple(sl)] for sl in slice_params]\n",
    "\n",
    "    def squeeze(self, dim=None):\n",
    "        if dim is None: return self if 1 not in self.shape else self.reshape(*[size for size in self.shape if size != 1])\n",
    "        if dim <= 0 and self.ndim == 0: return self # This is to match PyTorch behavior\n",
    "        if not -self.ndim <= dim < self.ndim: raise IndexError(f\"Dimension out of range (expected to be in range of [{-self.ndim if self.ndim > 0 else self.ndim-1}, {self.ndim-1 if self.ndim > 0 else self.ndim}], but got {dim})\")\n",
    "        if dim < 0: dim += self.ndim\n",
    "        return self if self.shape[dim] != 1 else self.reshape(*[size for idx, size in enumerate(self.shape) if idx != dim])\n",
    "\n",
    "    def unsqueeze(self, dim):\n",
    "        if dim < 0: dim = len(self.shape) + dim + 1\n",
    "        return self.reshape(self.shape[:dim] + (1,) + self.shape[dim:])\n",
    "\n",
    "    # (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    def pad2d(self, padding:Union[List[int], Tuple[int, ...]], value:float=0):\n",
    "        slc = [(-p0, s+p1) for p0,p1,s in zip(padding[::2], padding[1::2], self.shape[::-1])][::-1]\n",
    "        return self.slice([(0,s) for s in self.shape[:-(len(padding)//2)]] + slc, value=value)\n",
    "\n",
    "    @property\n",
    "    def T(self) -> Tensor: return self.transpose()\n",
    "    def transpose(self, ax1=1, ax2=0) -> Tensor:\n",
    "        order = list(range(len(self.shape)))\n",
    "        order[ax1], order[ax2] = order[ax2], order[ax1]\n",
    "        return self.permute(order)\n",
    "\n",
    "    def flatten(self, start_dim=0):\n",
    "        return self.reshape(shape=self.shape[:start_dim] + (-1,))\n",
    "\n",
    "    # ! reduce ops ====================\n",
    "\n",
    "    def _reduce(self, fxn:Type[Function], axis:Optional[Union[int, Tuple[int, ...]]]=None, keepdim=False) -> Tensor:\n",
    "        axis_: List[int] = list(range(len(self.shape))) if axis is None else ([axis] if axis.__class__ is int else list(axis)) # type: ignore\n",
    "        axis_ = [x if x >= 0 else x+len(self.shape) for x in axis_]\n",
    "        shape = [s for i,s in enumerate(self.shape) if i not in axis_]\n",
    "        ret = fxn.apply(self, new_shape=tuple([1 if i in axis_ else s for i,s in enumerate(self.shape)]))\n",
    "        return ret if keepdim else ret.reshape(shape=shape)\n",
    "\n",
    "    def sum(self, axis=None, keepdim=False): return self._reduce(mlops.Sum, axis, keepdim)\n",
    "    def max(self, axis=None, keepdim=False): return self._reduce(mlops.Max, axis, keepdim)\n",
    "    def min(self, axis=None, keepdim=False): return -((-self).max(axis=axis, keepdim=keepdim))\n",
    "\n",
    "    def mean(self, axis=None, keepdim=False):\n",
    "        assert all_int(self.shape), \"does not support symbolic shape\"\n",
    "        out = self.sum(axis=axis, keepdim=keepdim)\n",
    "        return out.mul(prod(out.shape)/prod(self.shape))\n",
    "\n",
    "    def std(self, axis=None, keepdim=False, correction=1):\n",
    "        assert all_int(self.shape), \"does not support symbolic shape\"\n",
    "        square_sum = ((self - self.mean(axis=axis, keepdim=True)).square()).sum(axis=axis, keepdim=keepdim)\n",
    "        return square_sum.div(prod(self.shape)/prod(square_sum.shape)-correction).sqrt()\n",
    "    \n",
    "    def _softmax(self, axis):\n",
    "        m = self - self.max(axis=axis, keepdim=True)\n",
    "        e = m.exp()\n",
    "        return m, e, e.sum(axis=axis, keepdim=True)\n",
    "    \n",
    "    def softmax(self, axis=-1):\n",
    "        _, e, ss = self._softmax(axis)\n",
    "        return e.div(ss)\n",
    "    \n",
    "    def log_softmax(self, axis=-1):\n",
    "        m, _, ss = self._softmax(axis)\n",
    "        return m - ss.log()\n",
    "\n",
    "    def argmax(self, axis=None, keepdim=False):\n",
    "        if axis is None:\n",
    "            idx = (self == self.max(axis)) * Tensor.arange(prod(self.shape)-1,-1,-1, dtype=dtypes.int32, requires_grad=False, device=self.device).reshape(self.shape)\n",
    "            return prod(self.shape) - idx.max() - 1\n",
    "        axis = axis + len(self.shape) if axis < 0 else axis\n",
    "        m = self == self.max(axis=axis, keepdim=True)\n",
    "        idx = m * Tensor.arange(self.shape[axis]-1,-1,-1, dtype=dtypes.int32, requires_grad=False, device=self.device).reshape(self.shape[axis], *[1]*(self.ndim-axis-1))\n",
    "        return self.shape[axis]-idx.max(axis=axis, keepdim=keepdim)-1\n",
    "\n",
    "    def argmin(self, axis=None, keepdim=False): return (-self).argmax(axis=axis, keepdim=keepdim)\n",
    "\n",
    "    # ! processing ops ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinygrad.helpers import argfix, prod\n",
    "\n",
    "# experiments\n",
    "\n",
    "argfix(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# lets learn walrus\n",
    "\n",
    "# https://www.python.org/dev/peps/pep-0572/\n",
    "\n",
    "# example 1\n",
    "if (len_ := len([1,2,3])) > 2:\n",
    "    print(len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n := len([1,2,3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
