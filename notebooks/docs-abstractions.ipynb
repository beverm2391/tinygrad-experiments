{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Tuple, Union, Any, Dict, Callable, Type, List, ClassVar\n",
    "from enum import Enum, auto\n",
    "from abc import ABC\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['DEBUG'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.ops import Device\n",
    "Device.DEFAULT = \"CLANG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.] + [3.] = [5.]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([2])\n",
    "b = Tensor([3])\n",
    "\n",
    "res = a + b\n",
    "print(f\"{a.numpy()} + {b.numpy()} = {res.numpy()}\")\n",
    "assert res.numpy()[0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad.mlops as mlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so tensor class time\n",
    "class Tensor:\n",
    "    # some class attributes\n",
    "    grad: Optional[Tensor] # grad is a tensor\n",
    "    requires_grad: Optional[bool]\n",
    "\n",
    "    # this is the graph for the autograd engine\n",
    "    _ctx: Optional[Function] # pretty sure we'll define this later\n",
    "\n",
    "    # this is where the data and other properties lice\n",
    "    lazydata: LazyBuffer\n",
    "\n",
    "    # high level ope (hlops) are defined on this class. ex: relu\n",
    "    def relu(self): return self.maximum(0)\n",
    "\n",
    "    # log is an mlp, this is the wrapper function in Tensor\n",
    "    def log(self): return mlops.Log.apply(self)\n",
    "\n",
    "# all the definitions of the derivatives are subclasses of Function (like mlops.Log)\n",
    "# there's only 18 mlops for derivatives for everything (in tinygrad/mlops.py)\n",
    "# read mlops.py and tensor.py --- they seem to be the core. maybe read mlops first?\n",
    "\n",
    "# heres autodiff\n",
    "class Function:\n",
    "    # exampole forward and backward methods\n",
    "    def forward(self, x:LazyBuffer) -> LazyBuffer: pass\n",
    "    def backward(self, x:LazyBuffer) -> LazyBuffer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.helpers import DType\n",
    "\n",
    "# this is where the properties live that I tought were part of the tensor class\n",
    "# LazyBuff is like a Tensor without derivatives, at the mlop layer\n",
    "\n",
    "class LazyBuffer:\n",
    "    # these three define the \"type\" of the buffer and are returned as Tensor properties\n",
    "    device: str\n",
    "    shape: Tuple[int, ...]\n",
    "    dtype: DType\n",
    "    # som eclas attrs\n",
    "\n",
    "\n",
    "    # a ShapeTracker tracks reshapes and permutes\n",
    "    # all MovementOps are zero copy \n",
    "    # the ShapeTracker specifies how the data in the RawBuffer matches to the shape\n",
    "    st: ShapeTracker\n",
    "\n",
    "    # if the LazyBuffer is realized, it has a RawBuffer\n",
    "    realized: Optional[RawBuffer]\n",
    "\n",
    "    # if the lazybuffer is unrealized, it has a LazyOp\n",
    "    # this is the comp needed to realize the LazyBuffer\n",
    "    op: Optional[LazyOp]\n",
    "\n",
    "# LazyOp\n",
    "# in a tree they form an AST for a single GPU kernel\n",
    "class LazyOp:\n",
    "    op: Op # they type of rthe compute\n",
    "    src: Tuple[Union[LazyOp, LazyBuffer], ...] # the sources\n",
    "    arg: Optional[Any] = None # the arguments\n",
    "\n",
    "# theres currently 28 ops you have to implement for an accelerator\n",
    "class UnaryOps(Enum): NOOP = auto(); EXP2 = auto(); LOG2 = auto(); CAST = auto(); SIN = auto()\n",
    "class BinaryOps(Enum):   ADD = auto();  SUB = auto();  MUL = auto();  DIV = auto();  CMPLT = auto(); MAX = auto()\n",
    "class ReduceOps(Enum):   SUM = auto();  MAX = auto()\n",
    "class MovementOps(Enum): RESHAPE = auto(); PERMUTE = auto(); EXPAND = auto(); PAD = auto(); SHRINK = auto(); STRIDE = auto()\n",
    "class TernaryOps(Enum):  MULACC = auto(); WHERE = auto()\n",
    "class LoadOps(Enum):     EMPTY = auto(); RAND = auto(); CONST = auto(); FROM = auto(); CONTIGUOUS = auto(); CUSTOM = auto()\n",
    "# If you have a compiledbuffer (devicebuffer)\n",
    "# you dont have to implement the MovementOps\n",
    "# as they are handles bu the ShapeTracker\n",
    "\n",
    "Op = Union[UnaryOps, BinaryOps, ReduceOps, MovementOps, TernaryOps, LoadOps]\n",
    "\n",
    "# most of tinygrad/lazy.py is concerned with fusing Ops into LasyOps ASTs that map to GPU kernels\n",
    "# it's beyond the scope of this tutorial but can read file if interested... maybe i will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tinygrad.lazy.LazyBuffer'> <LB (1,) dtypes.float op=BinaryOps.ADD st=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))>\n"
     ]
    }
   ],
   "source": [
    "# Example LazyBuffer for 2+3\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.ops import LazyOp, BinaryOps, LoadOps\n",
    "\n",
    "# 2 + 3 from before\n",
    "res = Tensor([2]) + Tensor([3])\n",
    "print(type(res.lazydata), res.lazydata)\n",
    "\n",
    "lazyop: LazyOp = res.lazydata.op\n",
    "assert lazyop.op == BinaryOps.ADD\n",
    "assert len(lazyop.src) == 2\n",
    "\n",
    "# first source is 2 which comes from the CPU\n",
    "# the source is a LazyBuffer that is a \"CPU\" Tensor\n",
    "# again, a LazyOp AST is like a GPU kerner. you have to copy the data on the device first\n",
    "assert lazyop.src[0].op.op == LoadOps.FROM\n",
    "assert lazyop.src[0].op.src[0].device == \"CPU\"\n",
    "assert lazyop.src[0].op.src[0].op.src[0].realized._buf[0] == 2, \"the src of the FROM LazyOP is a LazyBuffer on the CPU holding [2.]\"\n",
    "assert res.lazydata.realized is None, \"the LazyBuffer is not realized yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB (1,) dtypes.float op=buffer<1, dtypes.float, 5020201360> st=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))> on CLANG with grad None>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert res.lazydata.realized is not None, \"the LazyBuffer is realized!\"\n",
    "# brings us to DeviceBuffer (the realized ClangBuffer is a subclass of DeviceBuffer)\n",
    "assert 'RawMallocBuffer' in str(type(res.lazydata.realized))\n",
    "# can copy the device buffer to CPU\n",
    "assert res.lazydata.realized.toCPU()[0] == 5, \"when put in numpy with toCPU, it's 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 1: <class 'TypeError'>: Don't know how to convert parameter 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m output \u001b[39m=\u001b[39m RawMallocBuffer(\u001b[39m1\u001b[39m, dtypes\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m program \u001b[39m=\u001b[39m ClangProgram(\u001b[39m\"\u001b[39m\u001b[39madd\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvoid add(float *a, float *b, float *c) \u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m *a = *b + *c; \u001b[39m\u001b[39m}}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m program(\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, [output, input_a, input_b])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mtoCPU())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X14sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39massert\u001b[39;00m output\u001b[39m.\u001b[39mtoCPU()[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mthe output is 5\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/runtime/ops_clang.py:81\u001b[0m, in \u001b[0;36mClangProgram.__call__\u001b[0;34m(self, global_size, local_size, wait, *args)\u001b[0m\n\u001b[1;32m     79\u001b[0m   args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_buf \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mmem_read(mu\u001b[39m.\u001b[39mreg_read(arm64_const\u001b[39m.\u001b[39mUC_ARM64_REG_X0), args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize \u001b[39m*\u001b[39m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize)\n\u001b[1;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfxn(\u001b[39m*\u001b[39;49m[x\u001b[39m.\u001b[39;49m_buf \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(x, RawMallocBuffer) \u001b[39melse\u001b[39;49;00m x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m args])\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m wait: \u001b[39mreturn\u001b[39;00m time\u001b[39m.\u001b[39mmonotonic()\u001b[39m-\u001b[39mst\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 1: <class 'TypeError'>: Don't know how to convert parameter 1"
     ]
    }
   ],
   "source": [
    "# you can either write an \"Interpreted\" backend or \"Compiled\" backend\n",
    "\n",
    "class Interpreted:\n",
    "    # they have a backing RawBufer\n",
    "    buffer: Type[RawBuffer]\n",
    "\n",
    "    # and they have a lookup table to functions for the ops\n",
    "    fxn_for_op: Dict[Op, Callable] = {\n",
    "        UnaryOps.EXP2: lambda x: np.exp2(x),\n",
    "        BinaryOps.ADD: lambda x, y: x + y\n",
    "    }\n",
    "\n",
    "# compiled bckends take a little more (ex: GPU and LLVM)\n",
    "class Compiled:\n",
    "    # they also have a backingRawBuffer\n",
    "    buffer: Type[RawBuffer]\n",
    "\n",
    "    # a code generator, which compiles the AST\n",
    "    codegen: Type[Linearizer]\n",
    "\n",
    "    # and a runtime, which runs the generated code\n",
    "    runtime: Type[Runtime]\n",
    "\n",
    "# runtime is what actually runs the kernels for a compiled backend\n",
    "class Runtime(ABC):\n",
    "    # name is the name of the function, and prg is the code\n",
    "    # the constuctor takes the code and compiles it\n",
    "    def __init__(self, name:str, prg:str): pass\n",
    "    # call runs the code on the bufs. NOTE: the output is always bufs[0], but this is kust a convention\n",
    "    def __call__(self, global_size:Optional[List[int]], local_size:Optional[List[int]], bufs:List[RawBuffer]): pass\n",
    "\n",
    "\n",
    "# Rawbuffer is where the data is actually held, its pretty close to just memory\n",
    "class RawBuffer(ABC):\n",
    "    # create an empty rawbuffer that holds size elements of type dtype\n",
    "    # buf is an opaque container class\n",
    "    def __init__(self, size:int, dtype:DType, buf:Any): raise NotImplementedError\n",
    "\n",
    "    # fromCPU is a classmethod that creates a RawBudder, its a classmethod since some runtimes are 0 copy\n",
    "    @classmethod\n",
    "    def fromCPU(cle:RawBuffer, x:np.ndarray) -> RawBuffer: raise NotImplementedError\n",
    "\n",
    "    # toCPU converts the RawBuffer to a numpy array with shape (size,). many backends are 0 copy here\n",
    "    def toCPU(self) -> np.ndarray: raise NotImplementedError\n",
    "\n",
    "# RawNumpyBuffer is a RawBuffer example for numpy. It's very simple... or so you say\n",
    "class RawNumpyBuffer(RawBuffer):\n",
    "    def __init__(self, buf: np.ndarray):\n",
    "        super().__init__(buf.size, dtypes.from_np(buf.dtype). buf)\n",
    "    @classmethod\n",
    "    def fromCPU(cls, x): return cls(x)\n",
    "    def toCPU(self): return self.buf\n",
    "\n",
    "# example 2+3 in raw clang \n",
    "\n",
    "# RawMallocBuffer is the simplest concrete version of RawBuffer (in tinygrad/ops.py)\n",
    "# its used for CLANG and LLVM backends\n",
    "# its just malloc(size * dtype.itemsize)\n",
    "from tinygrad.runtime.lib import RawMallocBuffer\n",
    "\n",
    "# ClangProgram is the simplest runtime\n",
    "# __init__ calls clang, and __call__ calls the function in the *.so outputted by clang\n",
    "# in CLANG, global_size and local_size are ignored\n",
    "from tinygrad.runtime.ops_clang import ClangProgram\n",
    "\n",
    "# a concrete example looks like this, this adds two size1 RawBuffer\n",
    "# first we create two numpy buffers containing 2 and 3\n",
    "# then we copy the numpy in to RawMallocBuffers\n",
    "# last, we create an empty output buffer\n",
    "from tinygrad.helpers import dtypes\n",
    "numpy_a, numpy_b = np.array([2], dtype=np.float32), np.array([3], dtype=np.float32)\n",
    "input_a, input_b = RawMallocBuffer.fromCPU(numpy_a), RawMallocBuffer.fromCPU(numpy_b)\n",
    "output = RawMallocBuffer(1, dtypes.float32)\n",
    "\n",
    "program = ClangProgram(\"add\", f\"void add(float *a, float *b, float *c) {{ *a = *b + *c; }}\")\n",
    "program(None, None, [output, input_a, input_b])\n",
    "print(output.toCPU())\n",
    "assert output.toCPU()[0] == 5, \"the output is 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 UOps.DEFINE_GLOBAL  : ptr.dtypes.float          []                               ('data0', dtypes.float)\n",
      "   1 UOps.CONST          : dtypes.float              []                               2.0\n",
      "   2 UOps.CONST          : dtypes.float              []                               3.0\n",
      "   3 UOps.ALU            : dtypes.float              [1, 2]                           BinaryOps.ADD\n",
      "   4 UOps.CONST          : dtypes.int32              []                               0\n",
      "   5 UOps.STORE          :                           [0, 4, 3]                        None\n"
     ]
    }
   ],
   "source": [
    "# Linarizer\n",
    "\n",
    "# the first sttep of transforming an ASI into code is to linearize it (think like topsort on the AST)\n",
    "# for that we use the Linearizer, which turns an AST into a list of (linear) UOps\n",
    "\n",
    "class UOps(Enum): LOOP = auto(); DEFINE_LOCAL = auto(); LOAD = auto(); ALU = auto(); CONST = auto(); ENDLOOP = auto(); STORE = auto()\n",
    "\n",
    "class UOp:\n",
    "    uop: UOps\n",
    "    dtype: Optional[DType]\n",
    "    vin: Tuple[UOp, ...]\n",
    "    arg: Any\n",
    "    num: int # UOps are unique\n",
    "\n",
    "class Linearizer:\n",
    "    # create the kernel with the AST\n",
    "    # NOTE: The AST containes the ComiledBuffers themselves as the root nodes. this will change\n",
    "    def __init__(self, ast:LazyOp): pass\n",
    "    def linearize(self): pass\n",
    "\n",
    "    uops: List[UOp] # the linearized UOps\n",
    "\n",
    "from tinygrad.tensor import Tensor\n",
    "result = Tensor(2).realize() + Tensor(3).realize()\n",
    "\n",
    "# use the real linearizer to liearize 2 + 3\n",
    "\n",
    "from tinygrad.codegen.linearizer import Linearizer\n",
    "sched = result.lazydata.schedule()\n",
    "linearizer = Linearizer(sched[-1].ast)\n",
    "linearizer.linearize()\n",
    "\n",
    "for uop in linearizer.uops: print(uop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "void E_n2(float* restrict data0) {\n",
      "  data0[0] = (2.0f+3.0f);\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "\n",
    "result = Tensor(2) + Tensor(3)\n",
    "\n",
    "# we have a global cache used by the JIT\n",
    "# from there, we can see the generated clang code\n",
    "\n",
    "from tinygrad.jit import CacheCollector\n",
    "CacheCollector.start() # enable the cache\n",
    "result.realize() # realize the result\n",
    "cache_saved = CacheCollector.finish() # disable the cash\n",
    "\n",
    "# theres 1 ASTRunner in the cache\n",
    "assert len(cache_saved) == 1\n",
    "prg, bufs, _ = cache_saved[0]\n",
    "\n",
    "print(prg.prg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(10, 10), strides=(10, 1), offset=0, mask=None, contiguous=True),))\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.shape.shapetracker import ShapeTracker\n",
    "\n",
    "# create a virtual (10, 10) Tensor. this is just a shape, there's no actual tensor\n",
    "a = ShapeTracker.from_shape((10, 10))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(10, 10), strides=(1, 10), offset=0, mask=None, contiguous=False),))\n"
     ]
    }
   ],
   "source": [
    "a = a.permute((1, 0))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(5, 2, 5, 2), strides=(2, 1, 20, 10), offset=0, mask=None, contiguous=False),))\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape((5,2,5,2))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(5, 2, 5, 2), strides=(2, 1, 20, 10), offset=0, mask=None, contiguous=False), View(shape=(100,), strides=(1,), offset=0, mask=None, contiguous=True)))\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape((100,))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((idx0%10)*10)+(idx0//10))\n"
     ]
    }
   ],
   "source": [
    "idx, _ = a.expr_idxs()\n",
    "print(idx.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((idx0%10)*10)+(idx0//10))\n"
     ]
    }
   ],
   "source": [
    "idx, _ = a.expr_idxs()\n",
    "print(idx.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(5, 2, 5, 2), strides=(2, 1, 20, 10), offset=0, mask=None, contiguous=False), View(shape=(100,), strides=(1,), offset=0, mask=None, contiguous=True)))\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeTracker(views=(View(shape=(5, 2, 5, 2), strides=(2, 1, 20, 10), offset=0, mask=None, contiguous=False), View(shape=(100,), strides=(1,), offset=0, mask=None, contiguous=True)))\n"
     ]
    }
   ],
   "source": [
    "a = a.simplify()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "invalid permute (1, 0) for (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mpermute((\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(a)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/shape/shapetracker.py:186\u001b[0m, in \u001b[0;36mShapeTracker.permute\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermute\u001b[39m(\u001b[39mself\u001b[39m, axis: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ShapeTracker:\n\u001b[0;32m--> 186\u001b[0m   \u001b[39mreturn\u001b[39;00m ShapeTracker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviews[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpermute(axis), ))\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/shape/view.py:84\u001b[0m, in \u001b[0;36mView.permute\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# pylint: disable=method-cache-max-size-none\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermute\u001b[39m(\u001b[39mself\u001b[39m, axis: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m View:\n\u001b[0;32m---> 84\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m x \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m x \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m axis), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minvalid permute \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(axis)) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(axis) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(axis) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt permute \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m   \u001b[39mreturn\u001b[39;00m View\u001b[39m.\u001b[39mcreate(\u001b[39mtuple\u001b[39m([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[a] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m axis]), \u001b[39mtuple\u001b[39m([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides[a] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m axis]), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset, \u001b[39mtuple\u001b[39m([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask[a] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m axis]) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: invalid permute (1, 0) for (100,)"
     ]
    }
   ],
   "source": [
    "a = a.permute((1, 0))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/docs-abstractions.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m a\u001b[39m.\u001b[39mcontiguous \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert a.contiguous == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n",
      "0 20\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.shape.symbolic import Variable\n",
    "\n",
    "# Variable is the base class from symbolic\n",
    "# its created with a name and a min and max (inclusive)\n",
    "a = Variable(\"a\", 0, 10)\n",
    "b = Variable(\"b\", 0, 10)\n",
    "\n",
    "# some math examples\n",
    "print((a*10).min, (a*10).max)\n",
    "print((a+b).min, (a+b).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a%10)\n"
     ]
    }
   ],
   "source": [
    "expr = (a + b*10) % 10\n",
    "print(expr.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a*2)\n",
      "0 20\n"
     ]
    }
   ],
   "source": [
    "expr = (a*40 + b) // 20\n",
    "print(expr.render())\n",
    "print(expr.min, expr.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
