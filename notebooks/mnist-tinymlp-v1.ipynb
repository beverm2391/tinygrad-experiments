{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import math\n",
    "import torch\n",
    "from time import perf_counter\n",
    "\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.state import get_parameters\n",
    "\n",
    "from lib.utils import get_mnist\n",
    "from tinygrad.nn.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_mnist(\"../data\") # these need to be tensors??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.l1 = Linear(input_dim, hidden_dim)\n",
    "        self.l2 = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.l2(self.l1(x).relu())\n",
    "    \n",
    "    def parameters(self):\n",
    "        return get_parameters(self.l1) + get_parameters(self.l2)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MLP({self.l1}, {self.l2})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataLoader:\n",
    "    def __init__(self, X: Union[np.ndarray, Tensor], Y: Union[np.ndarray, Tensor], batch_size=64, shuffle=True):\n",
    "        self.X = Tensor(X) if not isinstance(X, Tensor) else X\n",
    "        self.Y = Tensor(Y) if not isinstance(Y, Tensor) else Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        xlen  = self.X.shape[0]\n",
    "        indices = np.random.permutation(xlen) if self.shuffle else np.arange(xlen) # shuffled indices if self.shuffle else range\n",
    "\n",
    "        for start_idx in range(0, xlen, self.batch_size):\n",
    "            end_idx = min(self.batch_size + start_idx, xlen)\n",
    "            batch_indices = Tensor(indices[start_idx:end_idx]) # this has to be tensor because indexing with np.ndarray or list raises error in tinygrad\n",
    "\n",
    "            yield self.X[batch_indices], self.Y[batch_indices]\n",
    "\n",
    "    def __len__(self): # return the number of batches\n",
    "        return math.ceil(self.X.shape[0] / self.batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = SimpleDataLoader(X_train, Y_train, batch_size=64, shuffle=True)\n",
    "test_loader = SimpleDataLoader(X_test, Y_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(784, 100, 10) # instantiate the model\n",
    "optim = SGD(model.parameters(), lr=0.001) # instantiate the optimizer\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = 1000 # num of batches per epoch\n",
    "BATCH_SIZE = 64\n",
    "max_batches_per_epoch = math.ceil(len(X_train) / BATCH_SIZE) # handle smaller last batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for mini-epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 782 Batches (max: 782) | Train Loss: 1.7502 | Test Accuracy: 0.4834 | Time: 35.00s\n",
      "Epoch 2/10: 782 Batches (max: 782) | Train Loss: 1.6234 | Test Accuracy: 0.6643 | Time: 35.51s\n",
      "Epoch 3/10: 782 Batches (max: 782) | Train Loss: 1.4638 | Test Accuracy: 0.7223 | Time: 37.48s\n",
      "Epoch 4/10: 782 Batches (max: 782) | Train Loss: 1.2776 | Test Accuracy: 0.7516 | Time: 31.91s\n",
      "Epoch 5/10: 782 Batches (max: 782) | Train Loss: 1.0900 | Test Accuracy: 0.7789 | Time: 32.24s\n",
      "Epoch 6/10: 782 Batches (max: 782) | Train Loss: 0.9340 | Test Accuracy: 0.8088 | Time: 32.11s\n",
      "Epoch 7/10: 782 Batches (max: 782) | Train Loss: 0.8068 | Test Accuracy: 0.8272 | Time: 30.76s\n",
      "Epoch 8/10: 782 Batches (max: 782) | Train Loss: 0.7138 | Test Accuracy: 0.8380 | Time: 33.51s\n",
      "Epoch 9/10: 782 Batches (max: 782) | Train Loss: 0.6417 | Test Accuracy: 0.8468 | Time: 36.18s\n",
      "Epoch 10/10: 782 Batches (max: 782) | Train Loss: 0.5916 | Test Accuracy: 0.8546 | Time: 33.53s\n",
      "Total training time: 338.24s\n"
     ]
    }
   ],
   "source": [
    "total_time = 0.0\n",
    "steps = min(STEPS, max_batches_per_epoch)\n",
    "for epoch in range(EPOCHS):\n",
    "    start = perf_counter()\n",
    "    running_train_loss = 0.0\n",
    "    for step in range(steps):\n",
    "        with Tensor.train():\n",
    "            samp = np.random.randint(0, X_train.shape[0], size=(64))\n",
    "\n",
    "            # get batch and labels\n",
    "            batch = Tensor(X_train[samp], requires_grad=False)\n",
    "            labels = Tensor(Y_train[samp])\n",
    "\n",
    "            out = model(batch) # forward pass\n",
    "            loss = out.sparse_categorical_crossentropy(labels) # calculate loss\n",
    "            optim.zero_grad() # zero out gradients\n",
    "            loss.backward() # backward pass\n",
    "            optim.step() # update weights\n",
    "\n",
    "            running_train_loss += loss.numpy()\n",
    "\n",
    "    train_loss = running_train_loss / STEPS # loss over all batches, over num batches\n",
    "\n",
    "    # test accuracy over the whole dataset\n",
    "    out = model(Tensor(X_test))\n",
    "    pred = out.argmax(axis=1) # get the index of the max value\n",
    "    accuracy = (pred == Tensor(Y_test)).mean().numpy()\n",
    "\n",
    "    elapsed = perf_counter() - start\n",
    "    total_time += elapsed\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}: {steps} Batches (max: {max_batches_per_epoch}) | Train Loss: {train_loss:.4f} | Test Accuracy: {accuracy:.4f} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"Total training time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP 10 Epochs MNIST**\n",
    "\n",
    "```\n",
    "Epoch 1/10: 782 Batches (max: 782) | Train Loss: 1.7502 | Test Accuracy: 0.4834 | Time: 35.00s\n",
    "Epoch 2/10: 782 Batches (max: 782) | Train Loss: 1.6234 | Test Accuracy: 0.6643 | Time: 35.51s\n",
    "Epoch 3/10: 782 Batches (max: 782) | Train Loss: 1.4638 | Test Accuracy: 0.7223 | Time: 37.48s\n",
    "Epoch 4/10: 782 Batches (max: 782) | Train Loss: 1.2776 | Test Accuracy: 0.7516 | Time: 31.91s\n",
    "Epoch 5/10: 782 Batches (max: 782) | Train Loss: 1.0900 | Test Accuracy: 0.7789 | Time: 32.24s\n",
    "Epoch 6/10: 782 Batches (max: 782) | Train Loss: 0.9340 | Test Accuracy: 0.8088 | Time: 32.11s\n",
    "Epoch 7/10: 782 Batches (max: 782) | Train Loss: 0.8068 | Test Accuracy: 0.8272 | Time: 30.76s\n",
    "Epoch 8/10: 782 Batches (max: 782) | Train Loss: 0.7138 | Test Accuracy: 0.8380 | Time: 33.51s\n",
    "Epoch 9/10: 782 Batches (max: 782) | Train Loss: 0.6417 | Test Accuracy: 0.8468 | Time: 36.18s\n",
    "Epoch 10/10: 782 Batches (max: 782) | Train Loss: 0.5916 | Test Accuracy: 0.8546 | Time: 33.53s\n",
    "Total training time: 338.24s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train For Full Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 1.41102050423927 | Test Loss: 1.2729565672054413\n",
      "Epoch 2/100 | Train Loss: 1.1991597955946423 | Test Loss: 1.0799631529552922\n",
      "Epoch 3/100 | Train Loss: 1.035741127436728 | Test Loss: 0.9349080020455038\n",
      "Epoch 4/100 | Train Loss: 0.9142745292705038 | Test Loss: 0.8270840436030346\n",
      "Epoch 5/100 | Train Loss: 0.8235563360669119 | Test Loss: 0.7458942454711647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist-conv-v1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist-conv-v1.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     out \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist-conv-v1.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msparse_categorical_crossentropy(y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist-conv-v1.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     running_test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist-conv-v1.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m | Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_train_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m | Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_test_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(test_loader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/tensor.py:125\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39massert\u001b[39;00m all_int(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno numpy if shape is symbolic, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mnp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno numpy dtype for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcast(dtypes\u001b[39m.\u001b[39;49mfrom_np(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype\u001b[39m.\u001b[39;49mnp))\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mCPU\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mrealize()\u001b[39m.\u001b[39mlazydata\u001b[39m.\u001b[39mrealized\u001b[39m.\u001b[39mtoCPU()\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/tensor.py:104\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrealize\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 104\u001b[0m   run_schedule(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazydata\u001b[39m.\u001b[39;49mschedule())\n\u001b[1;32m    105\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/realize.py:25\u001b[0m, in \u001b[0;36mrun_schedule\u001b[0;34m(schedule, disable_logging)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m si\u001b[39m.\u001b[39mast\u001b[39m.\u001b[39mop \u001b[39min\u001b[39;00m LoadOps:\n\u001b[1;32m     23\u001b[0m   \u001b[39m# confirm the LoadOps are contiguous and in order\u001b[39;00m\n\u001b[1;32m     24\u001b[0m   \u001b[39mfor\u001b[39;00m i,s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(si\u001b[39m.\u001b[39mast\u001b[39m.\u001b[39msrc): \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(s, LazyOp) \u001b[39mand\u001b[39;00m s\u001b[39m.\u001b[39mop \u001b[39m==\u001b[39m BufferOps\u001b[39m.\u001b[39mMEM \u001b[39mand\u001b[39;00m s\u001b[39m.\u001b[39marg\u001b[39m.\u001b[39midx \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m s\u001b[39m.\u001b[39marg\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39mcontiguous, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbad LoadOps src \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m   LOAD_OPS_DISPATCHER[cast(LoadOps, si\u001b[39m.\u001b[39;49mast\u001b[39m.\u001b[39;49mop)](si\u001b[39m.\u001b[39;49mout, \u001b[39m*\u001b[39;49msi\u001b[39m.\u001b[39;49minputs)\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m   si\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mrealized \u001b[39m=\u001b[39m Device[si\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mdevice]\u001b[39m.\u001b[39mexec_ast(si\u001b[39m.\u001b[39mast, output\u001b[39m=\u001b[39msi\u001b[39m.\u001b[39mout, inputs\u001b[39m=\u001b[39msi\u001b[39m.\u001b[39minputs, var_vals\u001b[39m=\u001b[39msi\u001b[39m.\u001b[39mvar_vals, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msi\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39m_device_extra_args())\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/realize.py:61\u001b[0m, in \u001b[0;36m_realize_from\u001b[0;34m(buffer, src)\u001b[0m\n\u001b[1;32m     58\u001b[0m   buffer\u001b[39m.\u001b[39mrealized \u001b[39m=\u001b[39m cast(RawBufferTransfer, Device[buffer\u001b[39m.\u001b[39mdevice]\u001b[39m.\u001b[39mbuffer)\u001b[39m.\u001b[39mtransfer(src\u001b[39m.\u001b[39mrealized, buffer\u001b[39m.\u001b[39mshape, buffer\u001b[39m.\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuffer\u001b[39m.\u001b[39m_device_extra_args())\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m   \u001b[39m# TODO: schedule this as FROM to go to CPU, and a FROM to go to device\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m   buffer\u001b[39m.\u001b[39mrealized \u001b[39m=\u001b[39m Device[buffer\u001b[39m.\u001b[39mdevice]\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mfromCPU(src\u001b[39m.\u001b[39;49mrealized\u001b[39m.\u001b[39;49mtoCPU(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuffer\u001b[39m.\u001b[39m_device_extra_args())\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/runtime/lib.py:41\u001b[0m, in \u001b[0;36mRawBufferMapped.toCPU\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoCPU\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray: \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfrombuffer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer(), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mnp, metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mbacking\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m}), count\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/runtime/ops_metal.py:35\u001b[0m, in \u001b[0;36mRawMetalBuffer._buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m   METAL\u001b[39m.\u001b[39;49msynchronize()\n\u001b[1;32m     36\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buf\u001b[39m.\u001b[39mcontents()\u001b[39m.\u001b[39mas_buffer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buf\u001b[39m.\u001b[39mlength())\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/runtime/ops_metal.py:26\u001b[0m, in \u001b[0;36m_METAL.synchronize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynchronize\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m   \u001b[39mfor\u001b[39;00m cbuf \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmtl_buffers_in_flight: cbuf\u001b[39m.\u001b[39;49mwaitUntilCompleted()\n\u001b[1;32m     27\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmtl_buffers_in_flight\u001b[39m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):    \n",
    "    with Tensor.train():\n",
    "        running_train_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            out = model(x)\n",
    "            loss = out.sparse_categorical_crossentropy(y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_train_loss += loss.numpy()\n",
    "        \n",
    "    running_test_loss = 0.0\n",
    "    for x, y in test_loader:\n",
    "        out = model(x)\n",
    "        loss = out.sparse_categorical_crossentropy(y)\n",
    "        running_test_loss += loss.numpy()\n",
    "\n",
    "    print(f\"Epoch {epoch+1/EPOCHS} | Train Loss: {running_train_loss/len(train_loader):0.4f} | Test Loss: {running_test_loss/len(test_loader):0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
