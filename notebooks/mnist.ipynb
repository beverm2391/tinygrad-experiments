{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "\n",
    "from lib.utils import get_mnist\n",
    "from tinygrad.nn.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_mnist(\"../data\") # these need to be tensors??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128, bias=False)\n",
    "        self.l2 = Linear(128, 10, bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.leakyrelu()\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyNet()\n",
    "opt = SGD([model.l1.weight, model.l2.weight], lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 2.2972 | Accuracy: 0.0938\n",
      "Step 101 | Loss: 2.2910 | Accuracy: 0.1719\n",
      "Step 201 | Loss: 2.2904 | Accuracy: 0.1562\n",
      "Step 301 | Loss: 2.2843 | Accuracy: 0.1875\n",
      "Step 401 | Loss: 2.2783 | Accuracy: 0.1250\n",
      "Step 501 | Loss: 2.2607 | Accuracy: 0.2500\n",
      "Step 601 | Loss: 2.2679 | Accuracy: 0.1406\n",
      "Step 701 | Loss: 2.2455 | Accuracy: 0.2500\n",
      "Step 801 | Loss: 2.2818 | Accuracy: 0.1250\n",
      "Step 901 | Loss: 2.2522 | Accuracy: 0.3594\n"
     ]
    }
   ],
   "source": [
    "with Tensor.train():\n",
    "    for step in range(1000):\n",
    "        # random sample a batch\n",
    "        samp = np.random.randint(0, X_train.shape[0], size=(64))\n",
    "        \n",
    "        # get batch and labels\n",
    "        batch = Tensor(X_train[samp], requires_grad=False)\n",
    "        labels = Tensor(Y_train[samp])\n",
    "\n",
    "        out = model(batch) # forward pass\n",
    "        loss = out.sparse_categorical_crossentropy(labels) # calculate loss\n",
    "        opt.zero_grad() # zero out gradients\n",
    "        loss.backward() # backward pass\n",
    "        opt.step() # update weights\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred = out.argmax(axis=-1)\n",
    "        acc = (pred == labels).mean()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step+1} | Loss: {loss.numpy():0.4f} | Accuracy: {acc.numpy():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyNet()\n",
    "opt = SGD([model.l1.weight, model.l2.weight], lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataLoader:\n",
    "    def __init__(self, X, Y, batch_size=64, shuffle=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.permutation(len(self.X)) # random indices\n",
    "        else:\n",
    "            indices = np.arange(len(self.X)) # ordered indices\n",
    "\n",
    "        for start_idx in range(0, len(self.X), self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, len(self.X))\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "            yield Tensor(self.X[batch_indices]), Tensor(self.Y[batch_indices])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = SimpleDataLoader(X_train, Y_train)\n",
    "testloader = SimpleDataLoader(X_test, Y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.2211 | Test Loss: 1.1682\n",
      "Epoch 2 | Train Loss: 1.1640 | Test Loss: 1.1115\n",
      "Epoch 3 | Train Loss: 1.1116 | Test Loss: 1.0596\n",
      "Epoch 4 | Train Loss: 1.0636 | Test Loss: 1.0121\n",
      "Epoch 5 | Train Loss: 1.0197 | Test Loss: 0.9689\n",
      "Epoch 6 | Train Loss: 0.9798 | Test Loss: 0.9295\n",
      "Epoch 7 | Train Loss: 0.9433 | Test Loss: 0.8936\n",
      "Epoch 8 | Train Loss: 0.9103 | Test Loss: 0.8608\n",
      "Epoch 9 | Train Loss: 0.8799 | Test Loss: 0.8308\n",
      "Epoch 10 | Train Loss: 0.8520 | Test Loss: 0.8034\n",
      "Epoch 11 | Train Loss: 0.8265 | Test Loss: 0.7783\n",
      "Epoch 12 | Train Loss: 0.8031 | Test Loss: 0.7550\n",
      "Epoch 13 | Train Loss: 0.7814 | Test Loss: 0.7337\n",
      "Epoch 14 | Train Loss: 0.7614 | Test Loss: 0.7139\n",
      "Epoch 15 | Train Loss: 0.7427 | Test Loss: 0.6956\n",
      "Epoch 16 | Train Loss: 0.7257 | Test Loss: 0.6787\n",
      "Epoch 17 | Train Loss: 0.7095 | Test Loss: 0.6629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msparse_categorical_crossentropy(y_batch) \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad() \u001b[39m# zero out gradients\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# backward pass\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m opt\u001b[39m.\u001b[39mstep() \u001b[39m# update weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/tinygrad-experiments/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m running_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/tensor.py:238\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m t0 \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepwalk()):\n\u001b[1;32m    237\u001b[0m   \u001b[39massert\u001b[39;00m (t0\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 238\u001b[0m   grads \u001b[39m=\u001b[39m t0\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mbackward(t0\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mlazydata)\n\u001b[1;32m    239\u001b[0m   grads \u001b[39m=\u001b[39m [Tensor(g, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m ([grads] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(t0\u001b[39m.\u001b[39m_ctx\u001b[39m.\u001b[39mparents) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m grads)]\n\u001b[1;32m    241\u001b[0m   \u001b[39mfor\u001b[39;00m t, g \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(t0\u001b[39m.\u001b[39m_ctx\u001b[39m.\u001b[39mparents, grads):\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/mlops.py:50\u001b[0m, in \u001b[0;36mRelu.backward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, grad_output:LazyBuffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LazyBuffer:\n\u001b[0;32m---> 50\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mret\u001b[39m.\u001b[39;49mconst(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39me(BinaryOps\u001b[39m.\u001b[39mCMPLT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mret)\u001b[39m.\u001b[39me(BinaryOps\u001b[39m.\u001b[39mMUL, grad_output)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/lazy.py:184\u001b[0m, in \u001b[0;36mLazyBuffer.const\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconst\u001b[39m(\u001b[39mself\u001b[39m, val:Union[\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LazyBuffer:\n\u001b[1;32m    183\u001b[0m   \u001b[39m# NOTE: dtypes.from_np(self.dtype.np) to deal with image types\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloadop(LoadOps\u001b[39m.\u001b[39;49mCONST, \u001b[39mtuple\u001b[39;49m(), dtypes\u001b[39m.\u001b[39;49mfrom_np(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype\u001b[39m.\u001b[39;49mnp), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, arg\u001b[39m=\u001b[39;49mval)\u001b[39m.\u001b[39;49mreshape((\u001b[39m1\u001b[39;49m,)\u001b[39m*\u001b[39;49m\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape))\u001b[39m.\u001b[39;49mexpand(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/lazy.py:280\u001b[0m, in \u001b[0;36mLazyBuffer.expand\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m arg: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mop \u001b[39m==\u001b[39m MovementOps\u001b[39m.\u001b[39mEXPAND: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39msrc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mexpand(arg)\n\u001b[0;32m--> 280\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_movement_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mst\u001b[39m.\u001b[39;49mexpand(arg), MovementOps\u001b[39m.\u001b[39;49mEXPAND, arg)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/lazy.py:262\u001b[0m, in \u001b[0;36mLazyBuffer._movement_op\u001b[0;34m(self, st, op, arg)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[39mif\u001b[39;00m root\u001b[39m.\u001b[39mst\u001b[39m.\u001b[39mcontiguous \u001b[39mand\u001b[39;00m root \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mand\u001b[39;00m prod(st\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m prod(root\u001b[39m.\u001b[39mshape):\n\u001b[1;32m    261\u001b[0m     \u001b[39mreturn\u001b[39;00m root\u001b[39m.\u001b[39mreshape(st\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m create_lazybuffer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, st, MovementOps, LazyOp(op, (\u001b[39mself\u001b[39;49m,), arg), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, base\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase)\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/lazy.py:96\u001b[0m, in \u001b[0;36mcreate_lazybuffer\u001b[0;34m(device, st, optype, op, dtype, base)\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m op\u001b[39m.\u001b[39mbuffers: x\u001b[39m.\u001b[39mchildren\u001b[39m.\u001b[39madd(lazycache[wop])\n\u001b[1;32m     94\u001b[0m   \u001b[39mreturn\u001b[39;00m lazycache[wop]\n\u001b[0;32m---> 96\u001b[0m lazycache[wop] \u001b[39m=\u001b[39m ret \u001b[39m=\u001b[39m LazyBuffer(device, st, optype, op, dtype, base\u001b[39m=\u001b[39;49mbase)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Documents/Coding/tinygrad-experiments/tinygrad/tinygrad/lazy.py:109\u001b[0m, in \u001b[0;36mLazyBuffer.__init__\u001b[0;34m(self, device, st, optype, op, dtype, src, base)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_buffer: Optional[RawBuffer] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m   \u001b[39m# TODO: do we really need this? or can we just use realized\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# TODO: does children have to be a ref count instead of a set? can a Buffer be a double child?\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren: WeakSet \u001b[39m=\u001b[39m WeakSet()\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews: WeakSet \u001b[39m=\u001b[39m WeakSet()\n\u001b[1;32m    111\u001b[0m \u001b[39m# NOTE: op should be read only after construction of LazyBuffer. it is now with schedule\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/_weakrefset.py:49\u001b[0m, in \u001b[0;36mWeakSet.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m# A list of keys to be removed\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_removals \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m()\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    with Tensor.train():\n",
    "        running_train_loss = 0.0\n",
    "        for x_batch, y_batch in trainloader:\n",
    "            out = model(x_batch) # forward pass\n",
    "            loss = out.sparse_categorical_crossentropy(y_batch) # calculate loss\n",
    "            opt.zero_grad() # zero out gradients\n",
    "            loss.backward() # backward pass\n",
    "            opt.step() # update weights\n",
    "\n",
    "            running_train_loss += loss.numpy()\n",
    "\n",
    "    running_test_loss = 0.0\n",
    "    for x_batch, y_batch in testloader:\n",
    "        out = model(x_batch)\n",
    "        loss = out.sparse_categorical_crossentropy(y_batch)\n",
    "        running_test_loss += loss.numpy()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {running_train_loss/len(trainloader):0.4f} | Test Loss: {running_test_loss/len(testloader):0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
