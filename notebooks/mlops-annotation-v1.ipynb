{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['DEBUG'] = '4' # set this to 4 for maximum verbosity - as seen in `/docs/env_vars.md`\n",
    "\n",
    "from typing import Tuple, Optional, cast\n",
    "from tinygrad.helpers import argsort, DType\n",
    "from tinygrad.ops import UnaryOps, BinaryOps, TernaryOps, ReduceOps\n",
    "from tinygrad.tensor import Function\n",
    "from tinygrad.lazy import LazyBuffer\n",
    "from tinygrad.shape.symbolic import sint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the core ml ops (28 i think?) that tinygrad uses\n",
    "# My plan is to read and annotate this file before working\n",
    "# upwards to Tensor\n",
    "# downwards to the actual ops (UnaryOps, BinaryOps, etc)\n",
    "# I might have to go take a trip to the shape.symbolic thing that build the shapetracker\n",
    "# but I'll cross that bridge when I get there. lets go.\n",
    "\n",
    "# Function is an abstract class that is the base class for all the ops\n",
    "# its defined in tensor.py and keeps track of stuff like the device, requires_grad, etc.\n",
    "# it has an `apply` method used to apply the function to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO - figure out why these are separate - maybe for functionality to be adde later?\n",
    "class Contiguous(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        return x.contiguous() # make sure data is stored in a contiguous chunk of memory\n",
    "    \n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        return grad_output # this is a no-op essentially\n",
    "\n",
    "class ContiguousBackward(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        return x # no-op\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        return grad_output.contiguous()\n",
    "\n",
    "# This is a function to cast the input to a different dtype\n",
    "class Cast(Function):\n",
    "    def forward(self, x: LazyBuffer, dtype: DType, bitcast: bool = False) -> LazyBuffer:\n",
    "        self.input_dtype, self.bitcast = x.dtype, bitcast \n",
    "        return x.cast(dtype, bitcast) # cast to the input dtype\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        return grad_output.cast(self.input_dtype, self.bitcast) # cast back to the input dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! Unary Ops ================================================================\n",
    "\n",
    "class Zero(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        return x.const(0) # return a tensor of zeros (const is a method of LazyBuffer)\n",
    "    \n",
    "    def backward(self, grad:LazyBuffer) -> LazyBuffer:\n",
    "        return grad.const(0) # zero out the gradient\n",
    "\n",
    "class Neg(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        return x.e(UnaryOps.NEG) # apply the unary op NEG to the input (e stands for elementwise)\n",
    "\n",
    "    def backward(self, grad: LazyBuffer) -> LazyBuffer:\n",
    "        return grad.e(UnaryOps.NEG) # apply the unary op NEG to the gradient\n",
    "    \n",
    "class Sin(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        self.x = x # store the input\n",
    "        return x.e(UnaryOps.SIN) # apply the unary op SIN to the input\n",
    "    \n",
    "    def backward(self, grad: LazyBuffer) -> LazyBuffer: # derivative\n",
    "        return self.x.const(math.pi / 2).e(BinaryOps.SUB, self.x).e(UnaryOps.SIN).e(BinaryOps.MUL, grad) # apply the chain rule\n",
    "\n",
    "# NOTE: maximum(x, 0) behaves differently where x=0 \n",
    "class ReLu(Function):\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = x.e(BinaryOps.MAX, x.const(0)) # apply the unary op RELU to the input\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        return self.ret.const(0).e(BinaryOps.CMPLT, self.ret).e(BinaryOps.MUL, grad_output) # apply the chain rule\n",
    "\n",
    "class Log(Function):\n",
    "    def forward(self, x:LazyBuffer) -> LazyBuffer:\n",
    "        self.x = x\n",
    "        return x.e(UnaryOps.LOG2).e(BinaryOps.MUL, x.const(math.log(2)))\n",
    "\n",
    "    def backward(self, grad_output:LazyBuffer) -> LazyBuffer:\n",
    "        return grad_output.e(BinaryOps.DIV, self.x)\n",
    "\n",
    "class Exp(Function):\n",
    "    def forward(self, x:LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = x.e(BinaryOps.MUL, x.const(1/math.log(2))).e(UnaryOps.EXP2)\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output:LazyBuffer) -> LazyBuffer:\n",
    "        return self.ret.e(BinaryOps.MUL, grad_output)\n",
    "\n",
    "class Sqrt(Function):\n",
    "    def forward(self, x:LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = x.e(UnaryOps.SQRT)\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output:LazyBuffer) -> LazyBuffer:\n",
    "        return grad_output.e(BinaryOps.DIV, self.ret.e(BinaryOps.MUL, self.ret.const(2)))\n",
    "    \n",
    "# NOTE: the implicit derivative of sigmoid is not stable\n",
    "# https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "# TODO: have the backend automatically find this \n",
    "class Sigmoid(Function):\n",
    "    def forward(self, x:LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = x.const(1).e(BinaryOps.DIV, x.const(1).e(BinaryOps.ADD, x.e(BinaryOps.MUL, x.const(-1/math.log(2))).e(UnaryOps.EXP2)))\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output:LazyBuffer) -> LazyBuffer:\n",
    "        return self.ret.e(BinaryOps.MUL, self.ret.const(1).e(BinaryOps.SUB, self.ret)).e(BinaryOps.MUL, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! Binary Ops ===============================================================\n",
    "\n",
    "class Less(Function):\n",
    "    def forward(self, x: LazyBuffer, y: LazyBuffer) -> LazyBuffer:\n",
    "        return x.e(BinaryOps.ADD, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
